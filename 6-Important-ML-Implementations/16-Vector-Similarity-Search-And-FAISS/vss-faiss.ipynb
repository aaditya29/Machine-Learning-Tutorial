{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eb100d",
   "metadata": {},
   "source": [
    "# Beginner Friendly Guide to Vector Similarity Search and Facebook AI Similarity Search(FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ff275",
   "metadata": {},
   "source": [
    "## 1. Embeddings: How Machines Understand Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1f9b8",
   "metadata": {},
   "source": [
    "> Computers don't understand **zero** and **one** but they understant **0** and **1**.\n",
    "\n",
    "When we type *“king”* into a computer, it doesn’t see royalty but instead it sees a string of characters such as `['k', 'i', 'n', 'g']`. To a machine that’s meaningless because t doesn’t know that *“king”* and *“queen”* are related or that *“king”* and *“banana”* are not.\n",
    "\n",
    "So if machines can’t understand meaning directly then\n",
    "**how do modern AI models like ChatGPT or search engines make sense of language?**<br>\n",
    "The answer is that they turn words and sentences into **numbers** that *capture meaning* and these hese numerical representations are called **embeddings**.\n",
    "\n",
    "\n",
    "### How Meanings Live in Space\n",
    "\n",
    "Imagine we have a huge map but instead of cities we place **words** on it such as:\n",
    "\n",
    "* Words that mean similar things are close together.\n",
    "* Words that mean different things are far apart.\n",
    "\n",
    "On this map we notice that\n",
    "\n",
    "> *“king” and “queen” live close together.*\n",
    "> *“apple” and “banana” live close together.*\n",
    "> *But “king” and “banana”? They live far apart.*\n",
    "\n",
    "That’s the intuition behind **embeddings**.\n",
    "They are coordinates of words, phrases or sentences in a high-dimensional **semantic space**.\n",
    "\n",
    "Every point (vector) represents the meaning of the text.\n",
    "\n",
    "### From Words to Vectors\n",
    "\n",
    "In the early days, we used something called **one-hot encoding**:\n",
    "Each word was a long vector of 0s with a single 1 marking its position.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Word   | One-hot vector (simplified) |\n",
    "| ------ | --------------------------- |\n",
    "| king   | [1, 0, 0, 0]                |\n",
    "| queen  | [0, 1, 0, 0]                |\n",
    "| apple  | [0, 0, 1, 0]                |\n",
    "| banana | [0, 0, 0, 1]                |\n",
    "\n",
    "This approach had a big problem:\n",
    "There are millions of words and all words are equally distant. There’s no sense of similarity between *“king”* and *“queen”*.\n",
    "\n",
    "### Enter Embeddings: Learning Meaning from Context\n",
    "\n",
    "Modern models (like Word2Vec, GloVe, and Transformer-based encoders) learn *dense* representations of words automatically by observing **context**.\n",
    "\n",
    "> The word *“bank”* in “river bank” vs. “bank account” appears in different neighborhoods.\n",
    "> The model learns those subtle differences.\n",
    "\n",
    "So instead of arbitrary one-hot vectors, we get something like:\n",
    "\n",
    "| Word   | Embedding (simplified)      |\n",
    "| ------ | --------------------------- |\n",
    "| king   | [0.61, 0.43, 0.72, 0.13, …] |\n",
    "| queen  | [0.59, 0.44, 0.70, 0.15, …] |\n",
    "| banana | [0.10, 0.87, 0.03, 0.91, …] |\n",
    "\n",
    "These numbers represent coordinates in a space where **distance = meaning difference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca8996",
   "metadata": {},
   "source": [
    "### Embeddings in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adeea61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(king, queen): 0.681\n",
      "Similarity(king, banana): 0.395\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings\n",
    "sentences = [\"king\", \"queen\", \"banana\"]\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarity between words\n",
    "sim_king_queen = util.cos_sim(embeddings[0], embeddings[1])\n",
    "sim_king_banana = util.cos_sim(embeddings[0], embeddings[2])\n",
    "\n",
    "print(f\"Similarity(king, queen): {sim_king_queen.item():.3f}\")\n",
    "print(f\"Similarity(king, banana): {sim_king_banana.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c816bfeb",
   "metadata": {},
   "source": [
    "These numbers tells us that “king” and “queen” are close in meaning whereas “king” and “banana” are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f39d0c",
   "metadata": {},
   "source": [
    "### Why Embeddings Matter\n",
    "\n",
    "Embeddings are the unsung backbone of modern NLP. They are useful in the\n",
    "* **Semantic Search:** finding meaning-based matches, not just keyword matches.\n",
    "* **Recommendation Systems:** suggesting content similar in theme.\n",
    "* **Chatbots & RAG:** retrieving relevant documents before answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538371b",
   "metadata": {},
   "source": [
    "## 2. Vector Similarity Search for Nearby Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6eca2",
   "metadata": {},
   "source": [
    "### Vector Based Methods for Similarity Search\n",
    "\n",
    "Earlier we learned that embeddings place words and sentences into a vector space where distance represents meaning. Now we want to know **how do we find the closest vectors to a query?** or it can be also written as **how do we search for similar text?**\n",
    "\n",
    "#### TF-IDF\n",
    "Imagine you have three documents:\n",
    "1. **Doc A:** The dog saw the cat.\n",
    "2. **Doc B:** The cat sat on the mat.\n",
    "3. **Doc C:** A fast brown fox jumps over the lazy dog.\n",
    "Now if you want to find a document about a **dog** how would you do it?\n",
    "\n",
    "If you are dumb like me then your first instinct might be to just count the words. This is called a \"Bag of Words\" approach.\n",
    "- Doc A: `{\"the\": 2, \"dog\": 1, \"saw\": 1, \"cat\": 1}`\n",
    "- Doc C: `{\"a\": 1, \"fast\": 1, \"brown\": 1, \"fox\": 1, \"jumps\": 1, \"over\": 1, \"the\": 1, \"lazy\": 1, \"dog\": 1}`\n",
    "\n",
    "A query for \"the dog\" would give Doc A a score of 3 (2 for \"the\", 1 for \"dog\") and Doc C a score of 2 (1 for \"the\", 1 for \"dog\"). But the problem here is that word **the** is completely useless and it’s somehow dominating our scores. This is where IDF comes into picture. <br>\n",
    "\n",
    "TF-IDF transforms documents into vectors based on term frequency (TF) and inverse document frequency (IDF). In layman terms we can write it as like words that appear often in one document but rarely in others get higher weight (they are more meaningful).<br>\n",
    "A word that appears many times in one article is probably important to that article. If we are reading an article about Python and the word Python appears 30 times it's a safe bet the article is about the programming language or can be of snake also.<br>\n",
    "\n",
    "$TF(word, document) = (Count of the word in the document) / (Total words in the document)$\n",
    "\n",
    "Hence in our above example \"The dog saw the cat,\" the TF for \"dog\" is 1/5 = 0.2. The TF for \"the\" is 2/5 = 0.4.\n",
    "\n",
    "But as we notice \"the\" is still winning by far. We've only solved half the problem. Now for the secret sauce.\n",
    "\n",
    "#### Inverse Document Frequency (IDF): How Special is this Word?\n",
    "\n",
    "Inverse Document Frequency  asks a simple question that how common is this word across all our documents? For example, Rare words are special. They are strong signifiers of a topic. The word \"quantum\" or \"bioinformatics\" is a fantastic keyword. Common words are noise. The word \"it,\" \"and,\" or \"is\" appears in almost every document. It tells us nothing. IDF gives a high score to rare words and a low score to common words.<br>\n",
    "It's calculated like this:<br>\n",
    "$IDF(word, all_documents) = log( (Total number of documents) / (Number of documents containing the word) )$\n",
    "\n",
    "Now looking at our example:<br>\n",
    "Total documents = 3\n",
    "- IDF(\"dog\"): Appears in 2 documents `(A, C)` -> `log(3 / 2)` = 0.17\n",
    "- IDF(\"cat\"): Appears in 2 documents `(A, B)` -> `log(3 / 2)` = 0.17\n",
    "- IDF(\"fox\"): Appears in 1 document (C) -> `log(3 / 1)` = 0.47\n",
    "- IDF(the): Appears in 3 documents (A, B, C). -> log(3 / 3) = log(1) = **0**\n",
    "\n",
    "Now finally log(1) becomes 0 completely silencing the word \"the\" but \"fox\" which is unique to Doc C gets the highest score.\n",
    "\n",
    "Now we just multiply them together to get the final TF-IDF score for every word in every document.<br>\n",
    "$TF-IDF = TF * IDF$\n",
    "\n",
    "Number obtained from this score is high if and only if the word is common in this document (High TF) and the word is rare across all other documents (High IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0a876",
   "metadata": {},
   "source": [
    "#### Implementing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b60f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"the dog saw the cat\".split()\n",
    "b = \"the cat saw the dog\".split()\n",
    "c = \"A fast brown fox jumps over the lazy dog.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877d9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tfidf(word):\n",
    "    tf = []\n",
    "    count_n = 0\n",
    "    for sentence in [a, b, c]:\n",
    "        # calculate TF\n",
    "        t_count = len([x for x in sentence if word in sentence])\n",
    "        tf.append(t_count/len(sentence))\n",
    "        # count number of docs for IDF\n",
    "        count_n += 1 if word in sentence else 0\n",
    "    idf = np.log10(len([a, b, c]) / count_n)\n",
    "    return [round(_tf*idf, 2) for _tf in tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bf7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF a: 0.18\n",
      "TF-IDF b: 0.18\n",
      "TF-IDF c: 0.0\n"
     ]
    }
   ],
   "source": [
    "tfidf_a, tfidf_b, tfidf_c = tfidf('dog')\n",
    "print(f\"TF-IDF a: {tfidf_a}\\nTF-IDF b: {tfidf_b}\\nTF-IDF c: {tfidf_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361191fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF a: 0.0\n",
      "TF-IDF b: 0.0\n",
      "TF-IDF c: 0.48\n"
     ]
    }
   ],
   "source": [
    "tfidf_a, tfidf_b, tfidf_c = tfidf('fox')\n",
    "print(f\"TF-IDF a: {tfidf_a}\\nTF-IDF b: {tfidf_b}\\nTF-IDF c: {tfidf_c}\")# changed word from 'dog' to 'forest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f5214",
   "metadata": {},
   "source": [
    "## 3. Facebook AI Similarity Search: How FAISS Finds a Needle in a Trillion Vector Haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852795a",
   "metadata": {},
   "source": [
    "In modern world everything is a vector. Our texts, photos or songs we hear we've gotten really good at turning them all into long lists of numbers called **embeddings.**<br>\n",
    "These vectors are powerful because they capture the meaning and context of the data. For example from BERT paper \"King\" - \"Man\" + \"Woman\" = \"Queen\" but it applies to everything.<br>\n",
    "- **Pictures:** Vectors for \"golden retriever in a park\" are \"close\" to vectors for \"dog on the grass.\"\n",
    "- **Text:** Vectors for \"How much is a flight to Tokyo?\" are \"close\" to \"Price of tickets to Japan.\"\n",
    "\n",
    "But this creates a new type of problem which we are calling **billion-vector problem**.<br>\n",
    "For example you are uploading a photo and we want to find he 10 most similar images then what is the most obvious nightmare choice?\n",
    "\n",
    "- Take the user's new image vector.\n",
    "- Compare it one by one to all the one billion vectors in our database (using a metric like Euclidean Distance or Cosine Similarity).\n",
    "- Sort the billion results by distance.\n",
    "- Return the top 10. \n",
    "\n",
    "This is called a brute-force or exhaustive search and it is completely accurate but it comes with a problem that it is also impossibly slow. You'd be waiting for minutes not milliseconds and if you are running a business then your users will leave then and servers will melt.<br>\n",
    "\n",
    "So what is solution of this? Here comes the **FAISS**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
