{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea767054",
   "metadata": {},
   "source": [
    "# Building GPT From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e1c28",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a886f16",
   "metadata": {},
   "source": [
    "### Reading the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d535d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2737fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a9db0",
   "metadata": {},
   "source": [
    "### Inspecting the Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d2fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9318e2",
   "metadata": {},
   "source": [
    "### Previewing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c593eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])# print the first 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1afe9",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b758b",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary\n",
    "A neural network can't process raw characters hence it needs numbers. Therefore our first step is to create a \"vocabulary\" of all the unique characters present in our text. We also calculate the `vocab_size` which is the number of unique characters. This will determine the size of our model's embedding and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4a93d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the unique characters in the dataset are as follows:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))# create a sorted list of unique characters\n",
    "vocab_size = len(chars)\n",
    "print(\"All the unique characters in the dataset are as follows: \", ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e04184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb4388",
   "metadata": {},
   "source": [
    "### Creating Encoder and Decoder\n",
    "\n",
    "Now we create a mapping (or \"tokenizer\") that can convert characters to integers and back.\n",
    "- **`stoi` (string-to-integer):** A dictionary that maps each unique character to a unique integer.\n",
    "- **`itos` (integer-to-string):** A dictionary that does the reverse, mapping integers back to characters.\n",
    "- `encode`: A function that takes a string and converts it into a list of integers (tokens) using stoi.\n",
    "- `decode`: A function that takes a list of integers and converts it back into a string using itos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da843a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers and vice versa\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a4188b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello world\")) # encode the string \"hello world\" to integers\n",
    "print(decode(encode(\"hello world\"))) # decode back to string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e7c27",
   "metadata": {},
   "source": [
    "### Encoding the Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f822372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)# encode the entire text dataset and store it in a torch tensor\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37189ce2",
   "metadata": {},
   "source": [
    "## 3. Data Splitting and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee94e6",
   "metadata": {},
   "source": [
    "### Train and Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2446eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))# first 90% will be train, rest validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271de848",
   "metadata": {},
   "source": [
    "### Understanding Context and Targets\n",
    "A language model works by predicting the next token in a sequence given a preceding context. The maximum length of this context is called the **block size** or **context length**.<br>\n",
    "Now we perform two tasks:\n",
    "- The input `x` is a chunk of text of length `block_size`.\n",
    "- The target `y` is the same chunk of text but shifted one position to the right.\n",
    "\n",
    "For every position `t` in the sequence the model will learn to predict the target `y[t]` using the context of all tokens up to `x[t]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceeda881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8# context length for predictions\n",
    "train_data[:block_size+1]# a chunk of text of length block_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5544466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]# input\n",
    "y = train_data[1:block_size+1]# target\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]# the context is everything up to and including position t\n",
    "    target = y[t]# the target is the next character we want to predict\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360ece6",
   "metadata": {},
   "source": [
    "### Creating a Batch Generation Function\n",
    "Training on the entire dataset at once is computationally expensive. Instead we train on small random chunks of data called minibatches.\n",
    "This function `get_batch` does the following:\n",
    "1. Selects `batch_size` (e.g., 4) random starting points in the dataset (`train_data` or `val_data`).\n",
    "2. For each starting point it grabs a sequence of length `block_size` for the input (`x`).\n",
    "3. It grabs the corresponding sequence of length block_size (shifted by one) for the target (y).\n",
    "4. It stacks these individual sequences into two tensors `xb` and `yb` of shape (batch_size, block_size).\n",
    "\n",
    "This provides a batch of independent examples that we can process in parallel which is highly efficient on modern hardware like GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c05da2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel\n",
    "block_size = 8 # what is the maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))# pick random starting indices for the batch \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])# gather the input sequences for each starting index\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])# gather the target sequences (shifted by one) for each starting index\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70b758e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [24] the target: 43\n",
      "When input is [24, 43] the target: 58\n",
      "When input is [24, 43, 58] the target: 5\n",
      "When input is [24, 43, 58, 5] the target: 57\n",
      "When input is [24, 43, 58, 5, 57] the target: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "When input is [44] the target: 53\n",
      "When input is [44, 53] the target: 56\n",
      "When input is [44, 53, 56] the target: 1\n",
      "When input is [44, 53, 56, 1] the target: 58\n",
      "When input is [44, 53, 56, 1, 58] the target: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52] the target: 58\n",
      "When input is [52, 58] the target: 1\n",
      "When input is [52, 58, 1] the target: 58\n",
      "When input is [52, 58, 1, 58] the target: 46\n",
      "When input is [52, 58, 1, 58, 46] the target: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "When input is [25] the target: 17\n",
      "When input is [25, 17] the target: 27\n",
      "When input is [25, 17, 27] the target: 10\n",
      "When input is [25, 17, 27, 10] the target: 0\n",
      "When input is [25, 17, 27, 10, 0] the target: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]# the context is everything up to and including position t\n",
    "        target = yb[b, t]# the target is the next character we want to predict\n",
    "        print(f\"When input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6817606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)# print the input batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abaae9",
   "metadata": {},
   "source": [
    "## 4. A Simple Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524725bb",
   "metadata": {},
   "source": [
    "### Creating A Bigram Model\n",
    "This model predicts the next token using only the single immediately preceding token. It has no memory of older tokens. This will serve as a baseline to measure our Transformer's improvement against.\n",
    "\n",
    "The model is implemented as a PyTorch `nn.Module`.\n",
    "- `nn.Embedding(vocab_size, vocab_size)`: This is the core of the model. It's a simple lookup table where for each of the `vocab_size` possible input tokens it stores a vocab_size-dimensional vector of logits (raw scores) for the next token.\n",
    "- `forward` **pass:**\n",
    "    - It takes the input batch `idx` and looks up the logits from the embedding table.\n",
    "    - It calculates the cross-entropy loss which is the standard loss function for classification tasks. It measures how well the predicted logits correspond to the actual target tokens.\n",
    "- `generate` **method:**\n",
    "    - This method produces new text autoregressively.\n",
    "    - It takes the current context idx gets the logits for the next token from the model applies softmax to convert logits to probabilities and then samples the next token from this probability distribution using torch.multinomial.\n",
    "    - This new token is then appended to the context and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4dd84d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape# batch size, time step, vocab size (channels)\n",
    "            logits = logits.view(B*T, C)# reshape to (B*T, C) for cross-entropy loss\n",
    "            targets = targets.view(B*T)# reshape to (B*T) for cross-entropy loss\n",
    "            loss = F.cross_entropy(logits, targets)# compute the cross-entropy loss\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)# instantiate the model\n",
    "logits, loss = m(xb, yb)# forward pass\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8428d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pJ:Bpm&yiltNCjeO3:Cx&vvMYW-txjuAd IRFbTpJ$zkZelxZtTlHNzdXXUiQQY:qFINTOBNLI,&oTigq z.c:Cq,SDXzetn3XVj\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee7e2d",
   "metadata": {},
   "source": [
    "### Training the Bigram Model\n",
    "Now we train our simple model using the **Adam Optimizer** a standard and effective optimization algorithm.\n",
    "The training loop is simple:\n",
    "1. Get a batch of data.\n",
    "2. Run the forward pass to get the logits and loss.\n",
    "3. Reset the gradients from the previous step `(optimizer.zero_grad())`.\n",
    "4. Perform the backward pass `(loss.backward())` to compute the gradients for all model parameters.\n",
    "5. Update the parameters using the optimizer `(optimizer.step())`.\n",
    "\n",
    "We repeat this for a number of steps. The loss should gradually decrease as the model learns the bigram statistics from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9637796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)# create an AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95b4dfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.554598808288574\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): #increase number of steps for good results\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)# reset the gradients\n",
    "    loss.backward()# backward pass to compute gradients\n",
    "    optimizer.step()# update the parameters\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dadb8784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wBM;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\n",
      "rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\n",
      "ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\n",
      "SV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\n",
      "tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\n",
      "pSPYgCuCJrIFtb\n",
      "jQXg\n",
      "pA.P LP,SPJi\n",
      "DBcuBM:CixjJ$Jzkq,OLI3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FsCV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\n",
      "D.?EGQhFttk!aUiZa!qB-pcL?OER:PAc'd,ip.SPyI-g:I'nviM;halgd\n",
      "dFIad,rA'b?qotd,!mJ.vcoibrIdZKtMb?s,SjKuBUzo-\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec478d",
   "metadata": {},
   "source": [
    "## 5. The Mathematical Trick in Self-Attention\n",
    "\n",
    "The core idea of a Transformer is **self-attention**. It's a mechanism that allows tokens in a sequence to communicate with and aggregate information from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f8646",
   "metadata": {},
   "source": [
    "### Toy Example: Weighted Aggregation\n",
    "\n",
    "This small example demonstrates the core trick.\n",
    "- Matrix `a` is a lower-triangular matrix of weights. The weights in each row sum to 1.\n",
    "- Matrix `b` contains our data vectors.\n",
    "- The matrix multiplication `c = a @ b` produces a new set of vectors `c` where each vector `c[i]` is a weighted average of the vectors `b[0]...b[i]`.\n",
    "\n",
    "This shows how matrix multiplication can efficiently perform a weighted sum over a sequence which is the fundamental operation in self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e5ec167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))# lower triangular matrix\n",
    "a = a / torch.sum(a, 1, keepdim=True)# normalize rows to sum to 1\n",
    "b = torch.randint(0, 10, (3, 2)).float()# random data matrix\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61683df5",
   "metadata": {},
   "source": [
    "### Self-Attention Version 1: for loop\n",
    "\n",
    "We have a batch of `B` sequences each of length `T` with `C` channels (embedding dimensions). We want each token `t` to be an average of all the tokens preceding it in its sequence.<br>\n",
    "Here we use `for` loops to iterate through the batch and the time steps. It works but it's very slow and inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aec044b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2# batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "257ff93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))# initialize the output tensor\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]# all previous time steps up to and including t\n",
    "        xbow[b, t] = torch.mean(xprev, 0)# average them\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4537d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ffa95549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea22955",
   "metadata": {},
   "source": [
    "### Self-Attention Version 2: Matrix Multiplication\n",
    "Here use the matrix multiplication trick from our toy example to replace the inner `for` loop.\n",
    "- `wei = torch.tril(...)`: We create a `(T, T)` lower-triangular weight matrix.\n",
    "- `xbow2 = wei @ x`: We matrix multiply the weights with our input data x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111b6318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))# (T,T) lower triangular matrix\n",
    "wei = wei / wei.sum(1, keepdim=True)# normalize rows to sum to 1\n",
    "xbow2 = wei @ x # matrix multiply the weights with the input data x (B,T,T)@(B,T,C) -> (B,T,C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c947d",
   "metadata": {},
   "source": [
    "### Self-Attention Version 3: Softmax\n",
    " Instead of creating the weights by simple division we'll use `softmax`.\n",
    "1. `tril = torch.tril(...)`: Create the lower-triangular matrix of ones and zeros.\n",
    "2. `wei = wei.masked_fill(...)`: We use `masked_fill` to replace all the 0s (the upper triangle) with negative infinity.\n",
    "3. `wei = F.softmax(wei, dim=-1)`: We apply softmax to this matrix. The softmax of negative infinity is 0 so this has the same effect as our previous masking but it's a more general mechanism that will be crucial for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3896bd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version3: using softmax\n",
    "tril = torch.tril(torch.ones(T, T))# (T,T) lower triangular matrix\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill((tril == 0), float('-inf'))# fill the upper triangular part with -inf\n",
    "wei = F.softmax(wei, dim=-1)# apply softmax to get the weights\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf095021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
