{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495f9c8f",
   "metadata": {},
   "source": [
    "# Building the GPT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb62bde",
   "metadata": {},
   "source": [
    "## 0. Some Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c45ca",
   "metadata": {},
   "source": [
    "### Introduction to Tokenization\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called **tokens**. These tokens are the vocabulary of the language model. \n",
    "\n",
    "#### Why is Tokenization so Important?\n",
    "\n",
    "- **Spelling, String Processing, and Arithmetic:** LLMs often struggle with tasks that seem simple to humans like spelling words correctly reversing a string, or performing basic math. This is because these tasks require operating on individual characters but the model sees the world in terms of \"tokens,\" which are often chunks of words. For example the number 1275 might be a single token not four separate digit characters making it hard for the model to perform arithmetic on it.\n",
    "\n",
    "- **Non-English Languages and Code:** The vocabulary of a tokenizer is typically trained on a massive corpus of text which is often predominantly English. This means other languages or programming languages might be broken down into less efficient or meaningful tokens making the model less effective at understanding and generating them.\n",
    "\n",
    "- **Weird Errors and Warnings:** Seemingly random failures or warnings like those related to \"trailing whitespace\" or specific strings like \"SolidGoldMagikarp\", are often artifacts of how the tokenizer processes text before the model ever sees it.\n",
    "\n",
    "- **The Dream of a Tokenizer-Free World:** A major goal in AI research is to create models that can operate directly on raw text (or even raw bytes) which would eliminate the entire layer of complexity and the associated problems introduced by tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f266a",
   "metadata": {},
   "source": [
    "### Unicode and UTF-8 Encoding\n",
    "\n",
    "- **Unicode:** This is a universal standard that assigns a unique number, called a code point to almost every character symbol or emoji in every language. \n",
    "- **UTF-8:** This is an encoding scheme that specifies how to represent these Unicode code points as a sequence of bytes (integers from 0 to 255). Simple characters like those in the English alphabet can be represented by a single byte while more complex characters or emojis might require multiple bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be51ba",
   "metadata": {},
   "source": [
    "## 1. Basic Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c18fc",
   "metadata": {},
   "source": [
    "### Unicode Code Points (ord)\n",
    "The `ord()` function in Python gives us the Unicode code point for a given character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff69ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83188493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x)for x in \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01680f2",
   "metadata": {},
   "source": [
    "### UTF-8 Encoded Bytes\n",
    "We take the same string and encode it into a sequence of bytes. \n",
    "- Notice that the English characters and symbols (like `h`, `e`, `l`, `o`,`     `) are represented by single bytes (numbers less than 128) that match their ASCII values.\n",
    "- The Korean characters and the emoji are represented by sequences of multiple bytes (numbers greater than 127). For example the waving hand emoji `ðŸ‘‹` becomes the four-byte sequence [240, 159, 145, 139]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587c6cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f19c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
