{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495f9c8f",
   "metadata": {},
   "source": [
    "# Building the GPT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb62bde",
   "metadata": {},
   "source": [
    "## 0. Some Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c45ca",
   "metadata": {},
   "source": [
    "### Introduction to Tokenization\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called **tokens**. These tokens are the vocabulary of the language model. \n",
    "\n",
    "#### Why is Tokenization so Important?\n",
    "\n",
    "- **Spelling, String Processing, and Arithmetic:** LLMs often struggle with tasks that seem simple to humans like spelling words correctly reversing a string, or performing basic math. This is because these tasks require operating on individual characters but the model sees the world in terms of \"tokens,\" which are often chunks of words. For example the number 1275 might be a single token not four separate digit characters making it hard for the model to perform arithmetic on it.\n",
    "\n",
    "- **Non-English Languages and Code:** The vocabulary of a tokenizer is typically trained on a massive corpus of text which is often predominantly English. This means other languages or programming languages might be broken down into less efficient or meaningful tokens making the model less effective at understanding and generating them.\n",
    "\n",
    "- **Weird Errors and Warnings:** Seemingly random failures or warnings like those related to \"trailing whitespace\" or specific strings like \"SolidGoldMagikarp\", are often artifacts of how the tokenizer processes text before the model ever sees it.\n",
    "\n",
    "- **The Dream of a Tokenizer-Free World:** A major goal in AI research is to create models that can operate directly on raw text (or even raw bytes) which would eliminate the entire layer of complexity and the associated problems introduced by tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f266a",
   "metadata": {},
   "source": [
    "### Unicode and UTF-8 Encoding\n",
    "\n",
    "- **Unicode:** This is a universal standard that assigns a unique number, called a code point to almost every character symbol or emoji in every language. \n",
    "- **UTF-8:** This is an encoding scheme that specifies how to represent these Unicode code points as a sequence of bytes (integers from 0 to 255). Simple characters like those in the English alphabet can be represented by a single byte while more complex characters or emojis might require multiple bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be51ba",
   "metadata": {},
   "source": [
    "## 1. Basic Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c18fc",
   "metadata": {},
   "source": [
    "### Unicode Code Points (ord)\n",
    "The `ord()` function in Python gives us the Unicode code point for a given character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff69ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 👋 (hello in Korean!)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"안녕하세요 👋 (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83188493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x)for x in \"안녕하세요 👋 (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01680f2",
   "metadata": {},
   "source": [
    "### UTF-8 Encoded Bytes\n",
    "We take the same string and encode it into a sequence of bytes. \n",
    "- Notice that the English characters and symbols (like `h`, `e`, `l`, `o`,`     `) are represented by single bytes (numbers less than 128) that match their ASCII values.\n",
    "- The Korean characters and the emoji are represented by sequences of multiple bytes (numbers greater than 127). For example the waving hand emoji `👋` becomes the four-byte sequence [240, 159, 145, 139]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587c6cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2a810",
   "metadata": {},
   "source": [
    "## 2. Implementing Byte-Pair Encoding\n",
    "\n",
    "The core idea of BPE is to start with a simple vocabulary (all individual bytes) and iteratively merge the most frequent pair of adjacent tokens to create new and longer tokens. This allows the model to learn a vocabulary that is optimized for the specific text it is trained on creating a balance between character-level and word-level tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb492c7",
   "metadata": {},
   "source": [
    "### Initial Text and Byte Conversion\n",
    "We start with a sample text and convert it into its raw UTF-8 byte representation. This list of integers (from 0 to 255) is our initial sequence of tokens. Our goal is to compress this token sequence compress by merging frequent pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f19c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode(\"utf-8\")# raw bytes\n",
    "# converting to a list of integers in range 0..255 for convenience\n",
    "tokens = list(map(int, tokens))\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))# number of characters\n",
    "print('---')\n",
    "print(tokens)# list of integers in range 0..255\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ef76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
