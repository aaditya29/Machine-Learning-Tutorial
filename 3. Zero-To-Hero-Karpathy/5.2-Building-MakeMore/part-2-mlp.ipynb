{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bc9f27",
   "metadata": {},
   "source": [
    "# Part-2: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e109ed4",
   "metadata": {},
   "source": [
    "Here we implement a multilayer perceptron (MLP) character-level language model using [Bengio et al. 2003 MLP language model paper.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779f4f9",
   "metadata": {},
   "source": [
    "### 1. Setting Up Environment and Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "457aaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c48d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()# reading the file and splitting into list of\n",
    "words[:8]# printing first 8 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7d4bd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)# number of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81460a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#building the vocabulary of characters\n",
    "chars = sorted(list(set(''.join(words))))# getting unique characters in the dataset and sorting them\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}# mapping characters to integers (1-26)\n",
    "stoi['.'] = 0# mapping the special character '.' to 0\n",
    "itos = {i:s for s,i in stoi.items()}# mapping integers to characters\n",
    "vocab_size = len(itos)# getting the vocabulary size\n",
    "print('vocab size: ', vocab_size)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5c8fa",
   "metadata": {},
   "source": [
    "We need to create a mapping from characters to integers `(stoi)` and back `(itos)`. This allows us to represent our text data numerically, which is the only format a neural network can understand.<br>\n",
    "Hence we create a sorted list of all unique characters and then add a special `.` token at index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8439088",
   "metadata": {},
   "source": [
    "- #### Building the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99229c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3# context length: how many characters do we take to predict the next one\n",
    "X, Y = [], []# input and output lists\n",
    "for w in words:\n",
    "    context = [0]*block_size# starting with a context of 3 '.' characters\n",
    "    for ch in w + '.':# for each character in the word plus the special character\n",
    "        ix = stoi[ch]# get the integer representation of the character\n",
    "        X.append(context)# append the current context to the input list\n",
    "        Y.append(ix)# append the integer representation of the character to the output list\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]# slide the context window to the right\n",
    "\n",
    "X = torch.tensor(X)# converting input list to tensor\n",
    "Y = torch.tensor(Y)# converting output list to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708aa16e",
   "metadata": {},
   "source": [
    "Instead of just looking at the single previous character (a bigram) we will now use a **context** of multiple characters to predict the next one.\n",
    "- `block_size = 3`: This is a crucial hyperparameter which defines our context length. Here block_size = 3 means our model will always use the last 3 characters to predict the 4th character.\n",
    "- We iterate through each word and build our input `X` and target `Y` pairs.\n",
    "    - For a name like \"emma\" the process is:\n",
    "        1. `...` ---> `e` (context is [0, 0, 0], target is e)\n",
    "        2. `..e` ---> `m` (context is [0, 0, e], target is m)\n",
    "        3. `.em` ---> `m` (context is [0, e, m], target is m)\n",
    "        4. `emm` ---> `a` (context is [e, m, a], target is a)\n",
    "        5. `mma` ---> `.` (context is [m, m, a], target is .)\n",
    "\n",
    "- `context = context[1:] + [ix]`: It is the \"sliding window.\" After each prediction we slide the context.\n",
    "\n",
    "> Finally X becomes a tensor where each row is a context of 3 character indices and Y is a tensor containing the corresponding target character index for each context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ec3877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90d41bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        ...,\n",
       "        [26, 26, 25],\n",
       "        [26, 25, 26],\n",
       "        [25, 26, 24]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c1efb",
   "metadata": {},
   "source": [
    "#### Creating Train, Validation and Test Splits\n",
    "\n",
    "- `Training Set (Xtr, Ytr)`: The largest chunk (80% of the data). The model learns the patterns from this data.\n",
    "- `Validation Set (Xdev, Ydev)`: A smaller portion (10%). We use this set to tune our model's hyperparameters (like embedding size, hidden layer size, learning rate) and to check for overfitting. The model does not train on this data.\n",
    "- `Test Set (Xte, Yte)`: The final 10%. We use this set only once at the very end to get an unbiased evaluation of how well our final, tuned model performs on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5f75f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "random.seed(42)# setting the seed for reproducibility\n",
    "random.shuffle(words)# shuffling the words\n",
    "n1 = int(0.8*len(words))# first 80% for training\n",
    "n2 = int(0.9*len(words))# next 10% for validation, last 10% for testing\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])# training set\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])# validation set\n",
    "Xte, Yte = build_dataset(words[n2:])# test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed7644",
   "metadata": {},
   "source": [
    "### 2. The Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d31171",
   "metadata": {},
   "source": [
    "#### The Embedding Layer\n",
    "\n",
    "Instead of using large sparse one-hot vectors, we'll use **embeddings**.\n",
    "\n",
    "- **What is it?** An embedding is a learned low-dimensional dense vector representation for each character in our vocabulary.\n",
    "- `C = torch.randn((27, 2))`: This creates our embedding matrix, which acts as a lookup table. It has 27 rows (one for each character) and 2 columns. The 2 means we are choosing to represent each character with a 2-dimensional vector (i.e., a point in a 2D plane). This embedding dimension is a hyperparameter we can tune.\n",
    "\n",
    "- **Benefit of using Embeddings:**\n",
    "    - It's much smaller than a 27-dimensional one-hot vector.\n",
    "    - The network will learn to place characters that are used in similar ways (like vowels or consonants) closer together in this 2D space therefore capturing semantic relationships that one-hot vectors cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c3de5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))# the embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44e8aa",
   "metadata": {},
   "source": [
    "Now we define how the embedding lookup works. `C[X]` uses the integer indices in X to retrieve the corresponding 2D embedding vectors from our matrix `C`.\n",
    "- Input `X` shape: `(228146, 3)` (228,146 examples each with a 3-char context).\n",
    "- Output `emb` shape: (228146, 3, 2) (For each of the 228,146 examples we now have the three 2D vectors corresponding to the three context characters with 2 being the size of the embedding vector for each character i.e. the embedding dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b64680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]# embedding the input characters\n",
    "emb.shape# shape of the embedded input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3a62c",
   "metadata": {},
   "source": [
    "#### Building Hidden Layer\n",
    "This is the first \"neuron\" layer of our MLP. It takes the embeddings as input and performs a non-linear transformation.\n",
    "- `W1 = torch.randn((6, 100))`: The weight matrix for the hidden layer.\n",
    "The input size is 6 because we have a 3-character context and each character is a 2D embedding (3 * 2 = 6). We will concatenate these embeddings to form a single 6D vector for each example.<br>\n",
    "The output size is 100, meaning our hidden layer will have 100 neurons. This is another hyperparameter.\n",
    "- `b1 = torch.randn(100)`: The bias vector for the 100 neurons in the hidden layer. Biases allow the neurons to shift their activation function, making them more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d751600",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1  = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e92d3",
   "metadata": {},
   "source": [
    "#### Hidden Layer Forward Pass\n",
    "This block computes the output of the hidden layer:\n",
    "- `emb.view(-1, 6)`: This is a crucial reshaping step. It takes our (228146, 3, 2) embedding tensor and transforms it into a (228146, 6) tensor. It does this by concatenating the three 2D vectors for each example into a single 6D vector. The -1 tells PyTorch to automatically infer the number of rows.<br>\n",
    "The `.view()` function in PyTorch reshapes a tensor to have different dimensions while keeping the total number of elements the same.\n",
    "\n",
    "```bash\n",
    "For the context ' . e m '\n",
    "[\n",
    "  [-0.1565,  0.1425],  # Embedding for '.'\n",
    "  [ 0.6479, -0.2573],  # Embedding for 'e'\n",
    "  [-0.0123,  0.3681]   # Embedding for 'm'\n",
    "]\n",
    "```\n",
    "When we apply .view(-1, 6), here's what each part does:\n",
    "1. `The 6`: This is the desired number of columns for our new tensor. We get 6 because we want to combine the features of our 3 context characters and each character has a 2-dimensional embedding.\n",
    "\n",
    "    - $3 (block_size) * 2 (embedding_dim) = 6$: This operation effectively takes the three 2D vectors and lays them end-to-end:\n",
    "    $[-0.1565,  0.1425,   0.6479, -0.2573,   -0.0123,  0.3681]$\n",
    "\n",
    "    - This new 6-dimensional vector is now a single, unified representation of the entire 3-character context.\n",
    "\n",
    "2. The `-1`: This is a powerful placeholder. It tells PyTorch**I want the number of columns to be 6 and you should automatically figure out how many rows are needed to make it work.**<br>\n",
    "PyTorch calculates this by taking the total number of elements in the tensor (32 * 3 * 2 = 192) and dividing by the specified number of columns (192 / 6 = 32). This ensures that our output tensor will have one row for each of the 32 examples in our minibatch.\n",
    "\n",
    "\n",
    "- `@ W1 + b1`: We perform a matrix multiplication with the weights and add the bias. This is the standard linear operation of a neuron layer.\n",
    "\n",
    "- `torch.tanh()`: We apply the hyperbolic tangent (tanh) activation function. Without a non-linear function between layers our multi-layer network would mathematically collapse into a single linear layer severely limiting its power. tanh squashes the output of each neuron to be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67da5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)# hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ba5482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9936, -0.9842, -0.9839,  ...,  0.9998, -0.9928, -0.1348],\n",
       "        [-0.8438, -0.9843, -0.9714,  ...,  0.9983, -0.9504, -0.8208],\n",
       "        [-0.9903, -0.9928, -0.8161,  ...,  0.9727,  0.9132,  0.3171],\n",
       "        ...,\n",
       "        [-0.9864, -0.9980, -0.9985,  ...,  0.9999, -0.9806, -0.9423],\n",
       "        [-1.0000, -0.9995, -0.9914,  ...,  0.9999, -0.5348,  0.7520],\n",
       "        [ 0.9990,  0.9968,  0.8851,  ...,  0.9821, -0.9999,  0.1843]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8695eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffd59b",
   "metadata": {},
   "source": [
    "#### The Output Layer\n",
    "This is the final layer of our network. It takes the activations from the hidden layer and produces the final output.\n",
    "- `W2 = torch.randn((100, 27))`: The weight matrix for the output layer.\n",
    "    - The input size is 100 matching the number of neurons in our hidden layer.\n",
    "    - The output size is 27 as we need one output number for each of the 27 possible next characters in our vocabulary.\n",
    "- `b2 = torch.randn(27)`: The bias vector for the 27 output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c49f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eece921",
   "metadata": {},
   "source": [
    "#### Output Layer Forward Pass(Logits)\n",
    "We perform the final matrix multiplication between the hidden layer activations `h` and the output weights `W2` and add the final bias `b2`. The result is our logits.<br>\n",
    "**Logits** are the raw and unnormalized scores for each of the 27 characters. They can be thought of as log-counts similar to the output of our linear layer but they are now produced by a much more powerful non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67082b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f2db854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb960d00",
   "metadata": {},
   "source": [
    "#### Converting Logits to Probabilities (Softmax)\n",
    "To turn our logits into a valid probability distribution, we apply the softmax function. This is a two-step process:\n",
    "- `counts = logits.exp()`: We exponentiate the logits. This makes all the numbers positive.\n",
    "- `prob = counts / counts.sum(1, keepdims=True)`: We normalize each row so that all the values in that row sum to 1.0.\n",
    "\n",
    "The prob tensor now contains for each input example the model's predicted probability for each of the 27 possible next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff259cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37279ff6",
   "metadata": {},
   "source": [
    "#### Calculating the Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48aa50e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6379)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d5b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
