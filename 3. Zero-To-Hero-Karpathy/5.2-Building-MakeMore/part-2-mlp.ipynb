{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bc9f27",
   "metadata": {},
   "source": [
    "# Part-2: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e109ed4",
   "metadata": {},
   "source": [
    "Here we implement a multilayer perceptron (MLP) character-level language model using [Bengio et al. 2003 MLP language model paper.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779f4f9",
   "metadata": {},
   "source": [
    "### 1. Setting Up Environment and Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457aaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c48d898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()# reading the file and splitting into list of\n",
    "words[:8]# printing first 8 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d4bd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)# number of words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81460a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#building the vocabulary of characters\n",
    "chars = sorted(list(set(''.join(words))))# getting unique characters in the dataset and sorting them\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}# mapping characters to integers (1-26)\n",
    "stoi['.'] = 0# mapping the special character '.' to 0\n",
    "itos = {i:s for s,i in stoi.items()}# mapping integers to characters\n",
    "vocab_size = len(itos)# getting the vocabulary size\n",
    "print('vocab size: ', vocab_size)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5c8fa",
   "metadata": {},
   "source": [
    "We need to create a mapping from characters to integers `(stoi)` and back `(itos)`. This allows us to represent our text data numerically, which is the only format a neural network can understand.<br>\n",
    "Hence we create a sorted list of all unique characters and then add a special `.` token at index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8439088",
   "metadata": {},
   "source": [
    "- #### Building the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99229c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3# context length: how many characters do we take to predict the next one\n",
    "X, Y = [], []# input and output lists\n",
    "for w in words:\n",
    "    context = [0]*block_size# starting with a context of 3 '.' characters\n",
    "    for ch in w + '.':# for each character in the word plus the special character\n",
    "        ix = stoi[ch]# get the integer representation of the character\n",
    "        X.append(context)# append the current context to the input list\n",
    "        Y.append(ix)# append the integer representation of the character to the output list\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]# slide the context window to the right\n",
    "\n",
    "X = torch.tensor(X)# converting input list to tensor\n",
    "Y = torch.tensor(Y)# converting output list to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708aa16e",
   "metadata": {},
   "source": [
    "Instead of just looking at the single previous character (a bigram) we will now use a **context** of multiple characters to predict the next one.\n",
    "- `block_size = 3`: This is a crucial hyperparameter which defines our context length. Here block_size = 3 means our model will always use the last 3 characters to predict the 4th character.\n",
    "- We iterate through each word and build our input `X` and target `Y` pairs.\n",
    "    - For a name like \"emma\" the process is:\n",
    "        1. `...` ---> `e` (context is [0, 0, 0], target is e)\n",
    "        2. `..e` ---> `m` (context is [0, 0, e], target is m)\n",
    "        3. `.em` ---> `m` (context is [0, e, m], target is m)\n",
    "        4. `emm` ---> `a` (context is [e, m, a], target is a)\n",
    "        5. `mma` ---> `.` (context is [m, m, a], target is .)\n",
    "\n",
    "- `context = context[1:] + [ix]`: It is the \"sliding window.\" After each prediction we slide the context.\n",
    "\n",
    "> Finally X becomes a tensor where each row is a context of 3 character indices and Y is a tensor containing the corresponding target character index for each context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec3877b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d41bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        ...,\n",
       "        [26, 26, 25],\n",
       "        [26, 25, 26],\n",
       "        [25, 26, 24]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c1efb",
   "metadata": {},
   "source": [
    "#### Creating Train, Validation and Test Splits\n",
    "\n",
    "- `Training Set (Xtr, Ytr)`: The largest chunk (80% of the data). The model learns the patterns from this data.\n",
    "- `Validation Set (Xdev, Ydev)`: A smaller portion (10%). We use this set to tune our model's hyperparameters (like embedding size, hidden layer size, learning rate) and to check for overfitting. The model does not train on this data.\n",
    "- `Test Set (Xte, Yte)`: The final 10%. We use this set only once at the very end to get an unbiased evaluation of how well our final, tuned model performs on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f75f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      # print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "random.seed(42)# setting the seed for reproducibility\n",
    "random.shuffle(words)# shuffling the words\n",
    "n1 = int(0.8*len(words))# first 80% for training\n",
    "n2 = int(0.9*len(words))# next 10% for validation, last 10% for testing\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])# training set\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])# validation set\n",
    "Xte, Yte = build_dataset(words[n2:])# test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed7644",
   "metadata": {},
   "source": [
    "### 2. The Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d31171",
   "metadata": {},
   "source": [
    "#### The Embedding Layer\n",
    "\n",
    "Instead of using large sparse one-hot vectors, we'll use **embeddings**.\n",
    "\n",
    "- **What is it?** An embedding is a learned low-dimensional dense vector representation for each character in our vocabulary.\n",
    "- `C = torch.randn((27, 2))`: This creates our embedding matrix, which acts as a lookup table. It has 27 rows (one for each character) and 2 columns. The 2 means we are choosing to represent each character with a 2-dimensional vector (i.e., a point in a 2D plane). This embedding dimension is a hyperparameter we can tune.\n",
    "\n",
    "- **Benefit of using Embeddings:**\n",
    "    - It's much smaller than a 27-dimensional one-hot vector.\n",
    "    - The network will learn to place characters that are used in similar ways (like vowels or consonants) closer together in this 2D space therefore capturing semantic relationships that one-hot vectors cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3de5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))# the embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44e8aa",
   "metadata": {},
   "source": [
    "Now we define how the embedding lookup works. `C[X]` uses the integer indices in X to retrieve the corresponding 2D embedding vectors from our matrix `C`.\n",
    "- Input `X` shape: `(228146, 3)` (228,146 examples each with a 3-char context).\n",
    "- Output `emb` shape: (228146, 3, 2) (For each of the 228,146 examples we now have the three 2D vectors corresponding to the three context characters with 2 being the size of the embedding vector for each character i.e. the embedding dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b64680a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]# embedding the input characters\n",
    "emb.shape# shape of the embedded input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3a62c",
   "metadata": {},
   "source": [
    "#### Building Hidden Layer\n",
    "This is the first \"neuron\" layer of our MLP. It takes the embeddings as input and performs a non-linear transformation.\n",
    "- `W1 = torch.randn((6, 100))`: The weight matrix for the hidden layer.\n",
    "The input size is 6 because we have a 3-character context and each character is a 2D embedding (3 * 2 = 6). We will concatenate these embeddings to form a single 6D vector for each example.<br>\n",
    "The output size is 100, meaning our hidden layer will have 100 neurons. This is another hyperparameter.\n",
    "- `b1 = torch.randn(100)`: The bias vector for the 100 neurons in the hidden layer. Biases allow the neurons to shift their activation function, making them more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d751600",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1  = torch.randn((6,100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e92d3",
   "metadata": {},
   "source": [
    "#### Hidden Layer Forward Pass\n",
    "This block computes the output of the hidden layer:\n",
    "- `emb.view(-1, 6)`: This is a crucial reshaping step. It takes our (228146, 3, 2) embedding tensor and transforms it into a (228146, 6) tensor. It does this by concatenating the three 2D vectors for each example into a single 6D vector. The -1 tells PyTorch to automatically infer the number of rows.<br>\n",
    "The `.view()` function in PyTorch reshapes a tensor to have different dimensions while keeping the total number of elements the same.\n",
    "\n",
    "```bash\n",
    "For the context ' . e m '\n",
    "[\n",
    "  [-0.1565,  0.1425],  # Embedding for '.'\n",
    "  [ 0.6479, -0.2573],  # Embedding for 'e'\n",
    "  [-0.0123,  0.3681]   # Embedding for 'm'\n",
    "]\n",
    "```\n",
    "When we apply .view(-1, 6), here's what each part does:\n",
    "1. `The 6`: This is the desired number of columns for our new tensor. We get 6 because we want to combine the features of our 3 context characters and each character has a 2-dimensional embedding.\n",
    "\n",
    "    - $3 (block_size) * 2 (embedding_dim) = 6$: This operation effectively takes the three 2D vectors and lays them end-to-end:\n",
    "    $[-0.1565,  0.1425,   0.6479, -0.2573,   -0.0123,  0.3681]$\n",
    "\n",
    "    - This new 6-dimensional vector is now a single, unified representation of the entire 3-character context.\n",
    "\n",
    "2. The `-1`: This is a powerful placeholder. It tells PyTorch**I want the number of columns to be 6 and you should automatically figure out how many rows are needed to make it work.**<br>\n",
    "PyTorch calculates this by taking the total number of elements in the tensor (32 * 3 * 2 = 192) and dividing by the specified number of columns (192 / 6 = 32). This ensures that our output tensor will have one row for each of the 32 examples in our minibatch.\n",
    "\n",
    "\n",
    "- `@ W1 + b1`: We perform a matrix multiplication with the weights and add the bias. This is the standard linear operation of a neuron layer.\n",
    "\n",
    "- `torch.tanh()`: We apply the hyperbolic tangent (tanh) activation function. Without a non-linear function between layers our multi-layer network would mathematically collapse into a single linear layer severely limiting its power. tanh squashes the output of each neuron to be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67da5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)# hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba5482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1250,  0.3907,  0.9784,  ..., -0.7286,  0.9744,  0.8753],\n",
       "        [ 0.9905, -0.8181,  0.9932,  ...,  0.9322,  0.8646, -0.0396],\n",
       "        [ 1.0000, -0.9996,  0.9977,  ...,  1.0000, -0.9910, -0.9971],\n",
       "        ...,\n",
       "        [ 0.9916, -1.0000,  0.9997,  ..., -0.3409,  0.9967,  0.9692],\n",
       "        [-0.9690,  0.8625,  0.9999,  ..., -0.6991, -0.9808,  0.9971],\n",
       "        [ 0.9992,  0.7552, -0.9954,  ..., -0.2669,  0.9743,  0.0947]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8695eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffd59b",
   "metadata": {},
   "source": [
    "#### The Output Layer\n",
    "This is the final layer of our network. It takes the activations from the hidden layer and produces the final output.\n",
    "- `W2 = torch.randn((100, 27))`: The weight matrix for the output layer.\n",
    "    - The input size is 100 matching the number of neurons in our hidden layer.\n",
    "    - The output size is 27 as we need one output number for each of the 27 possible next characters in our vocabulary.\n",
    "- `b2 = torch.randn(27)`: The bias vector for the 27 output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c49f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eece921",
   "metadata": {},
   "source": [
    "#### Output Layer Forward Pass(Logits)\n",
    "We perform the final matrix multiplication between the hidden layer activations `h` and the output weights `W2` and add the final bias `b2`. The result is our logits.<br>\n",
    "**Logits** are the raw and unnormalized scores for each of the 27 characters. They can be thought of as log-counts similar to the output of our linear layer but they are now produced by a much more powerful non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67082b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f2db854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb960d00",
   "metadata": {},
   "source": [
    "#### Converting Logits to Probabilities (Softmax)\n",
    "To turn our logits into a valid probability distribution, we apply the softmax function. This is a two-step process:\n",
    "- `counts = logits.exp()`: We exponentiate the logits. This makes all the numbers positive.\n",
    "- `prob = counts / counts.sum(1, keepdims=True)`: We normalize each row so that all the values in that row sum to 1.0.\n",
    "\n",
    "The prob tensor now contains for each input example the model's predicted probability for each of the 27 possible next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff259cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37279ff6",
   "metadata": {},
   "source": [
    "#### Calculating the Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48aa50e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.2525)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(prob.shape[0]), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb1f68",
   "metadata": {},
   "source": [
    "### 3. Full Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716d5b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99081b19",
   "metadata": {},
   "source": [
    "#### Initializing All Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e226bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,10), generator=g)# character embeddings\n",
    "W1 = torch.randn((30,200), generator=g)# first layer weights\n",
    "b1 = torch.randn(200, generator=g)# first layer bias\n",
    "W2 = torch.randn((200,27), generator=g)# second layer weights\n",
    "b2 = torch.randn(27, generator=g)# second layer bias\n",
    "parameters = [C, W1, b1, W2, b2]# list of all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc749d2",
   "metadata": {},
   "source": [
    "Here we initialize all the parameters of our network in one place. These are the numbers the model will learn during training.\n",
    "- `C (Embedding Matrix)`: 27 characters each with a 10-dimensional embedding vector.\n",
    "- `W1, b1 (Hidden Layer)`: 10 * 3 = 30 inputs, 200 hidden neurons.\n",
    "- `W2, b2 (Output Layer)`: 200 inputs (from hidden layer) 27 outputs (for each character).\n",
    "- `parameters`: We group all these tensors into a list for easier management during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cb1b5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)# number of parameters in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c264c",
   "metadata": {},
   "source": [
    "#### Enabling gradient tracking\n",
    "We loop through all our parameters and set `requires_grad = True`. This tells PyTorch to track the computational graph for these tensors so it can automatically compute gradients during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e09e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True# enabling gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1c5a3",
   "metadata": {},
   "source": [
    "#### Setting Up Learning Rates\n",
    "Here we create a range of learning rates logarithmically spaced from $10^{-3} (0.001)$ to $10^{0} (1.0)$.\n",
    "\n",
    "- `lre = torch.linspace(-3, 0, 1000)`: This line creates a tensor named lre (learning rate exponent) containing 1000 evenly spaced numbers between -3 and 0.\n",
    "- `lrs = 10**lre`: This line takes 10 to the power of each exponent in the lre tensor. This transforms the linear scale of exponents (-3, -2.99, ... 0) into a logarithmic scale of actual learning rates (10⁻³, ..., 10⁰), which is 0.001 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec33aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f0d36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []# learning rate index\n",
    "lossi = []# loss values\n",
    "stepi = []# step index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea4ab6",
   "metadata": {},
   "source": [
    "#### The Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4106597900390625\n"
     ]
    }
   ],
   "source": [
    "for i in range(200000):\n",
    "    #construct the minibatch for each iteration\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))# random minibatch of 32 samples\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]# embed the characters into vectors (32, 3, 10)\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)# hidden layer (32, 200)\n",
    "    logits = h @ W2 + b2# output layer (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])# cross-entropy loss\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None# setting the gradients to zero\n",
    "    loss.backward()# backpropagation\n",
    "    \n",
    "    #update the parameters\n",
    "    #lr = lrs[i]# learning rate\n",
    "    lr = 0.1 if i < 100000 else 0.01# using a fixed learning rate schedule\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad# gradient descent step\n",
    "    \n",
    "    #lri.append(lre[i].item())# appending the learning rate index\n",
    "    stepi.append(i)# appending the step index\n",
    "    lossi.append(loss.log10().item())# appending the loss value\n",
    "\n",
    "print(loss.item())# printing the final loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b821fb2",
   "metadata": {},
   "source": [
    "This is where we train our MLP using gradient descent.\n",
    "- **Looping:** We run the optimization for 200,000 steps.\n",
    "- **Minibatch Construction:** In each step we don't use the entire dataset. Instead we randomly select a small minibatch of 32 examples (`ix = torch.randint(...)`). This is a standard technique called Stochastic Gradient Descent (SGD).\n",
    "- **Forward Pass:**\n",
    "    1. `emb = C[Xtr[ix]]`: Get the embeddings for the 32 examples in our minibatch.\n",
    "    2. `h = torch.tanh(...)`: Compute the hidden layer activations.\n",
    "    3. `logits = h @ W2 + b2`: Compute the final logits.\n",
    "    4. `loss = F.cross_entropy(logits, Ytr[ix])`: Calculate the cross-entropy loss for this minibatch. This function is a highly optimized combination of the softmax and negative log likelihood steps.\n",
    "\n",
    "- **Backward Pass:**\n",
    "    1. `p.grad = None`: Reset all gradients to zero.\n",
    "    2. `loss.backward()`: Compute the gradients of the loss with respect to all parameters.\n",
    "\n",
    "- **Update:**\n",
    "    1. `lr = 0.1 if i < 100000 else 0.01`: This is our learning rate schedule. We use a larger learning rate (0.1) for the first half of training to make big progress and then \"decay\" it to a smaller rate (0.01) for the second half to fine-tune the weights more carefully.\n",
    "    2. `p.data += -lr * p.grad`: Update each parameter using its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2a3ac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13baac1c0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449feee",
   "metadata": {},
   "source": [
    "This shows that our model learned quickly at the beginning and then converged to a good solution. The small bump at 100,000 steps is where we decreased the learning rate causing a temporary instability before the model settled into a new phase of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30a018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
