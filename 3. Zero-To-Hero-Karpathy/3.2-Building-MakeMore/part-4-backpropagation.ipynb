{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6441c8a",
   "metadata": {},
   "source": [
    "# Part 4: Becoming a Backpropagation Ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304fe414",
   "metadata": {},
   "source": [
    "Here we take the 2-layer MLP (with `BatchNorm`) from the previous part and backpropagate through it manually without using PyTorch autograd's `loss.backward()` through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8e135",
   "metadata": {},
   "source": [
    "## 1. Starter Codes from Previous Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ba3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fcf0d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4d86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49619ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "import random\n",
    "block_size = 3  # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4bcd7",
   "metadata": {},
   "source": [
    "## 2. Setting Up for Manual Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccf5f7",
   "metadata": {},
   "source": [
    "### Utility Function for Gradient Comparison\n",
    "This helper function `cmp` will be our sanity check. It compares a manually calculated gradient (dt) with the gradient calculated automatically by PyTorch (t.grad).\n",
    "\n",
    "It checks for:\n",
    "- `exact`: Whether the tensors are bit-for-bit identical.\n",
    "- `approximate`: Whether they are very close in value (useful for floating-point comparisons).\n",
    "- `maxdiff`: The maximum absolute difference between any two corresponding elements in the tensors.\n",
    "\n",
    "We'll use this function after each step of our manual backpropagation to verify that our calculations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9bef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()# exact\n",
    "  app = torch.allclose(dt, t.grad)# approximate\n",
    "  maxdiff = (dt - t.grad).abs().max().item()# max difference\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746c926",
   "metadata": {},
   "source": [
    "### Initializing Parameters\n",
    "We initialize all the parameters for our 2-layer MLP including the weights and biases for each layer and the gain/bias for Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8196062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * \\\n",
    "    (5/3)/((n_embd * block_size)**0.5)\n",
    "# using b1 just for fun, it's useless because of BN\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4227a04",
   "metadata": {},
   "source": [
    "### Constructing a Minibatch\n",
    "We create a single minibatch of 32 examples (`Xb`, `Yb`) from our training set. We will perform one full forward and backward pass on this single batch to analyze the gradient calculations at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdcdb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f458f1",
   "metadata": {},
   "source": [
    "### The Updated Forward Pass and PyTorch Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9f1ba1",
   "metadata": {},
   "source": [
    "Here we set up exercise performs a complete forward pass but instead of writing it in a few compact lines where every single intermediate calculation is stored in its own variable.\n",
    "- **Why break it down?** By storing every result (e.g. `hprebn`, `bndiff`, `bnraw`, `logits`, `probs`) we can analyze the gradient of the loss with respect to each of these intermediate variables. This allows us to manually backpropagate the gradients one step at a time.\n",
    "\n",
    "- **Numerical Stability:**\n",
    "\n",
    "    - `norm_logits = logits - logit_maxes`: Subtracting the maximum value from the logits before exponentiating is a standard trick to prevent numerical instability. `exp()` of large positive numbers can result in infinity (`inf`) but this trick keeps the inputs to `exp()` at or below zero.\n",
    "\n",
    "    - `counts_sum_inv = counts_sum**-1`: Using `x**-1` instead of `1.0 / x` can sometimes lead to more numerically precise gradients in PyTorch which is important when we want to check for exact equality.\n",
    "    \n",
    "- `t.retain_grad()`: This command tells PyTorch to save the gradient for intermediate non-leaf variables (like `h`, `logits`, `probs`). Normally PyTorch only saves gradients for the leaf nodes (our parameters). We need these intermediate gradients to check our work.\n",
    "\n",
    "- `loss.backward()`: This is PyTorch's automatic differentiation engine in action. It calculates the gradients for all variables in one go. We run it here to get the \"correct answers\" that we will compare our manual calculations against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f767e2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3461, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]  # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)# batch mean\n",
    "bndiff = hprebn - bnmeani# batch difference from mean\n",
    "bndiff2 = bndiff**2# batch squared difference from mean\n",
    "# note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5# inverse standard deviation\n",
    "bnraw = bndiff * bnvar_inv# normalized batch\n",
    "hpreact = bngain * bnraw + bnbias# affine transform\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)  # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()# unnormalized probabilities\n",
    "counts_sum = counts.sum(1, keepdims=True)# normalization constant\n",
    "counts_sum_inv = counts_sum**-1# inverse of normalization constant\n",
    "probs = counts * counts_sum_inv# probabilities for each class\n",
    "logprobs = probs.log()# log-probabilities for each class\n",
    "loss = -logprobs[range(n), Yb].mean()# average negative log-likelihood loss\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,  # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bf436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
