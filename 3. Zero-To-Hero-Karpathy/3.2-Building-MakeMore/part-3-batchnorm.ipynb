{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a7aaf1",
   "metadata": {},
   "source": [
    "# Part-3: Activations, Gradients and BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47dcc8",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this part we dive into the practical challenges of training neural networks and introduces powerful techniques like Batch Normalization to make our models train faster and better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b37c91",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbef743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ead416",
   "metadata": {},
   "source": [
    "### Reading the Names Dataset\n",
    "\n",
    "Like previous parts we load the names.txt file into a Python list called words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b817ea09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines() # list of strings\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33cc105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc24cf9",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "We create our character-to-integer `(stoi)` and integer-to-character `(itos)` mappings. This vocabulary is essential for converting our text data into a numerical format for the network. We also define `vocab_size` which we'll need for the dimensions of our output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f1cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))# unique characters in the dataset\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}# char to int mapping\n",
    "stoi['.'] = 0# we will use '.' as a special character to represent \"end of string\"\n",
    "itos = {i: s for s, i in stoi.items()}# int to char mapping\n",
    "vocab_size = len(itos)# vocab size\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6026ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcefdf",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e08f12",
   "metadata": {},
   "source": [
    "### Creating the Dataset (Train/Val/Test Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea1b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3#context length: how many characters do we take to predict the next one?\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []# X is the input, Y is the target\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size# initialize the context with 'block_size' zeros\n",
    "    for ch in w + '.':# we add the special character '.' to the end of each word\n",
    "      ix = stoi[ch]# get the integer representation of the character\n",
    "      X.append(context)# append the current context to X\n",
    "      Y.append(ix)# append the target character to Y\n",
    "      context = context[1:] + [ix]# slide the context window and add the new character\n",
    "\n",
    "  X = torch.tensor(X)# convert to tensor\n",
    "  Y = torch.tensor(Y)# convert to tensor\n",
    "  print(X.shape, Y.shape)# print the shape of X and Y\n",
    "  return X, Y# return the dataset\n",
    "\n",
    "\n",
    "random.seed(42)# for reproducibility\n",
    "random.shuffle(words)# shuffle the words\n",
    "n1 = int(0.8*len(words))#80%\n",
    "n2 = int(0.9*len(words))#90%\n",
    "\n",
    "Xtr,  Ytr = build_dataset(words[:n1])#80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])#10%\n",
    "Xte,  Yte = build_dataset(words[n2:])#10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25c976",
   "metadata": {},
   "source": [
    "## 3. MLP with Batch Normalization\n",
    "\n",
    "### Summary of Batch Normalisation\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique used in neural networks to make training faster and more stable. It works by normalising the inputs to each layer during training to have a consistent mean and standard deviation, which helps the network learn more efficiently.<br>\n",
    "\n",
    "#### Problem in the Learning Environment\n",
    "\n",
    "- **Covariate Shift:** Covariate shift is a fundamental challenge in machine learning that occurs when the distribution of our input features (also called covariates) is different in the real world (inference) than it was in your training data.<br>\n",
    "The model learns patterns from one specific type of data but then fails when it sees slightly different data in production even if the underlying task is the same.\n",
    "\n",
    "- **Internal Covariate Shift:** As the network learns the weights in the previous layers are constantly updated. This means the distribution of the data that each layer receives is also constantly changing. A layer might learn to expect inputs with a certain mean and range but in the next training step those inputs might have a completely different mean and range.<br>\n",
    "This phenomenon is called **Internal Covariate Shift.** It makes training unstable and slow because each layer is constantly trying to adapt to a new input distribution.\n",
    "\n",
    "#### Solution by Normalizing Each Batch\n",
    "BatchNorm solves this problem by adding a step that standardizes the inputs to a layer for each mini-batch of data. Before the data is passed through an activation function (like ReLU or tanh), BatchNorm ensures the data for that specific batch has:\n",
    "- A mean of 0.\n",
    "- A standard deviation of 1.\n",
    "This gives each layer a stable predictable distribution of inputs at every training step making the learning process much smoother and faster. It's like giving each layer a pair of auto-adjusting sunglasses so it always sees a consistently lit world, no matter how the \"lights\" (the weights of the previous layer) are flickering.\n",
    "\n",
    "\n",
    "> I will suggest go through the linked video for deeper understanding of [BatchNorm.](https://www.youtube.com/watch?v=2AscwXePInA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0ae62",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa80e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
