{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb0cc90",
   "metadata": {},
   "source": [
    "# PyTorch Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c92030",
   "metadata": {},
   "source": [
    "## Typical PyTorch Workflow Summary\n",
    "\n",
    "A typical PyTorch workflow is a structured and five-step process for training a neural network. The best way to understand it is through an analogy of**teaching a student to pass an exam**.\n",
    "\n",
    "Here’s the entire process at a glance:\n",
    "\n",
    "1.  **Prepare the Data** (Get the study materials)\n",
    "2.  **Build the Model** (The student's brain)\n",
    "3.  **Define Loss Function & Optimizer** (The practice test and study technique)\n",
    "4.  **Create the Training Loop** (The study session)\n",
    "5.  **Evaluate the Model** (The final exam)\n",
    "\n",
    "\n",
    "#### 1. Prepare the Data (The Study Materials)\n",
    "\n",
    "Before a student can learn they need textbooks and notes. Similarly a model needs data. This step involves getting the data and preparing it for PyTorch.\n",
    "\n",
    "* **What we do:** We load our data (images, text, numbers) and convert it into **tensors**, which are the fundamental data structure in PyTorch.\n",
    "* **Key PyTorch tools:**\n",
    "    * `torch.utils.data.Dataset`: An object that holds our entire dataset. Think of it as the complete textbook.\n",
    "    * `torch.utils.data.DataLoader`: An object that wraps our `Dataset` and serves up the data in small and manageable **batches**. A student doesn't study the entire textbook at once but they study chapter by chapter. The `DataLoader` hence provides these \"chapters.\"\n",
    "\n",
    "#### 2. Build the Model (The Student's Brain)\n",
    "\n",
    "Next we need to define the student—the neural network itself. It's like building a brain with layers of neurons which are ready to learn but it doesn't know anything yet.\n",
    "\n",
    "* **What we do:** We define yur neural network architecture as a Python class.\n",
    "* **Key PyTorch tools:**\n",
    "    * `torch.nn.Module`: All of our models will be subclasses of this. It gives us all the core functionality.\n",
    "    * `__init__()`: In this methodw you define the layers of our network (e.g., `torch.nn.Linear` for simple layers, `torch.nn.Conv2d` for image layers). This is like defining the different parts of the brain.\n",
    "    * `forward()`: This method defines how data flows through the layers you created. It's the path of thinking.\n",
    "\n",
    "#### 3. Define the Loss Function & Optimizer (The Test & Study Technique)\n",
    "\n",
    "How does a student know if they're learning? They take a practice test. And how do they improve? They use a study technique.\n",
    "\n",
    "* **Loss Function (The Practice Test):** This measures how wrong our model's predictions are compared to the true answers. A high loss means the model is very wrong and a low loss means it's doing well.\n",
    "    * **PyTorch examples:** `nn.CrossEntropyLoss` (for classification) or `nn.MSELoss` (for regression).\n",
    "* **Optimizer (The Study Technique):** This is the algorithm that updates the model's internal parameters (its \"knowledge\") to reduce the loss. It tells the model *how* to learn from its mistakes.\n",
    "    * **PyTorch examples:** `torch.optim.Adam` or `torch.optim.SGD`.\n",
    "\n",
    "#### 4. The Training Loop (The Study Session)\n",
    "\n",
    "This is the core of the learning process where everything comes together. We'll loop over our data multiple times (called **epochs**) and in each loop, the model will practice, get graded, and improve.<br>\n",
    "A single training step inside the loop has five parts:\n",
    "\n",
    "1.  **Forward Pass:** The model makes a prediction on a batch of data. (`model(inputs)`)\n",
    "    * *Analogy: The student answers a set of practice questions.*\n",
    "2.  **Calculate Loss:** The loss function compares the model's prediction to the correct answer. (`loss_fn(...)`)\n",
    "    * *Analogy: The practice test is graded yielding a score.*\n",
    "3.  **Zero Gradients:** We reset the optimizer's memory of past errors. (`optimizer.zero_grad()`)\n",
    "    * *Analogy: The student clears their mind before tackling the next set of questions.*\n",
    "4.  **Backpropagation:** PyTorch calculates how much each parameter in the model contributed to the error. (`loss.backward()`)\n",
    "    * *Analogy: The student figures out *why* they got each question wrong.*\n",
    "5.  **Update Weights:** The optimizer adjusts the model's parameters based on the backpropagation results. (`optimizer.step()`)\n",
    "    * *Analogy: The student corrects their understanding and updates their knowledge.*\n",
    "\n",
    "You repeat this loop over and over until the model's performance stops improving.\n",
    "\n",
    "\n",
    "#### 5. Evaluate the Model (The Final Exam)\n",
    "\n",
    "After the study sessions are over it's time for the final exam. We test the model on **new and unseen data** to see how well it truly learned to generalize.\n",
    "\n",
    "* **What we do:** We create a testing loop. It's similar to the training loop but with two key differences:\n",
    "    1.  We use `model.eval()` mode to turn off certain layers that behave differently during training (like dropout).\n",
    "    2.  We use `with torch.no_grad():` to tell PyTorch not to calculate gradients which makes evaluation faster and more memory-efficient. We're only testing not learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
