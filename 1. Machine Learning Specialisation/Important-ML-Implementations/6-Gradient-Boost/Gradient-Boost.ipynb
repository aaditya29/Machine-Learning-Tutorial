{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is an ensemble machine learning technique that combines the predictions of several base models (typically decision trees) to produce a more accurate and robust model. The \"boosting\" part refers to the way these models are built sequentially, each one trying to correct the errors of the previous one.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Ensemble Learning**: Combining the predictions of multiple models to improve the overall performance. Gradient boosting is a type of ensemble learning.\n",
    "\n",
    "2. **Boosting**: A sequential technique where each new model is trained to correct the errors made by the previous models.\n",
    "\n",
    "3. **Gradient Descent**: An optimization algorithm used to minimize the error by adjusting the model parameters. In gradient boosting, gradient descent is used to minimize the loss function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Gradient Boosting\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction. This could be as simple as predicting the mean value of the target variable for regression problems.\n",
    "\n",
    "2. **Calculate Residuals**: Compute the difference between the actual values and the predicted values. These differences are called residuals.\n",
    "\n",
    "3. **Train Weak Learner**: Train a new model (often a decision tree) on the residuals. The goal is for this new model to predict the residuals.\n",
    "\n",
    "4. **Update Predictions**: Add the predictions from the new model to the previous predictions to get the updated predictions.\n",
    "\n",
    "5. **Repeat**: Repeat steps 2-4 for a specified number of iterations or until the model performance stops improving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Gradient Boosting for Regression\n",
    "\n",
    "Going through a simple example to illustrate the process:\n",
    "\n",
    "#### Step 1: Initialization\n",
    "\n",
    "Suppose we have a dataset with features $(X)$ and target variable $(y)$. We start with an initial prediction $( \\hat{y}_0 )$, which could be the mean of \\( y \\).\n",
    "\n",
    "$[ \\hat{y}_0 = \\frac{1}{n} \\sum_{i=1}^{n} y_i ]$\n",
    "\n",
    "#### Step 2: Calculate Residuals\n",
    "\n",
    "Calculate the residuals (errors) for each instance in the dataset.\n",
    "\n",
    "$[ r_i = y_i - \\hat{y}_0 ]$\n",
    "\n",
    "#### Step 3: Train Weak Learner\n",
    "\n",
    "Train a decision tree on the residuals $( r )$. The decision tree will learn to predict the residuals.\n",
    "\n",
    "#### Step 4: Update Predictions\n",
    "\n",
    "Update the predictions by adding the predictions from the decision tree to the initial predictions.\n",
    "\n",
    "$[ \\hat{y}_1 = \\hat{y}_0 + \\text{tree}_1(X) ]$\n",
    "\n",
    "#### Step 5: Repeat\n",
    "\n",
    "Repeat steps 2-4 for a specified number of iterations. Each iteration adds another decision tree that corrects the errors of the previous ensemble.\n",
    "\n",
    "### Gradient Boosting Parameters\n",
    "\n",
    "1. **Number of Trees (n_estimators)**: The number of decision trees to be added. More trees can improve accuracy but also increase computation time and risk overfitting.\n",
    "\n",
    "2. **Learning Rate**: A scaling factor applied to each tree's contribution. Lower values make the model more robust to overfitting but require more trees.\n",
    "\n",
    "3. **Tree Depth**: Controls the complexity of each decision tree. Deeper trees can capture more information but also risk overfitting.\n",
    "\n",
    "### Popular Gradient Boosting Libraries\n",
    "\n",
    "1. **XGBoost**: An optimized gradient boosting library that is efficient and highly scalable.\n",
    "2. **LightGBM**: A gradient boosting framework that uses tree-based learning algorithms, designed to be efficient and fast.\n",
    "3. **CatBoost**: A gradient boosting library that handles categorical features well and is known for its ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GradientBoost Algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
