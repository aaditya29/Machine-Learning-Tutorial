{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boost Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is an ensemble machine learning technique that combines the predictions of several base models (typically decision trees) to produce a more accurate and robust model. The \"boosting\" part refers to the way these models are built sequentially, each one trying to correct the errors of the previous one.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Ensemble Learning**: Combining the predictions of multiple models to improve the overall performance. Gradient boosting is a type of ensemble learning.\n",
    "\n",
    "2. **Boosting**: A sequential technique where each new model is trained to correct the errors made by the previous models.\n",
    "\n",
    "3. **Gradient Descent**: An optimization algorithm used to minimize the error by adjusting the model parameters. In gradient boosting, gradient descent is used to minimize the loss function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Gradient Boosting\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction. This could be as simple as predicting the mean value of the target variable for regression problems.\n",
    "\n",
    "2. **Calculate Residuals**: Compute the difference between the actual values and the predicted values. These differences are called residuals.\n",
    "\n",
    "3. **Train Weak Learner**: Train a new model (often a decision tree) on the residuals. The goal is for this new model to predict the residuals.\n",
    "\n",
    "4. **Update Predictions**: Add the predictions from the new model to the previous predictions to get the updated predictions.\n",
    "\n",
    "5. **Repeat**: Repeat steps 2-4 for a specified number of iterations or until the model performance stops improving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Gradient Boosting for Regression\n",
    "\n",
    "Going through a simple example to illustrate the process:\n",
    "\n",
    "#### Step 1: Initialization\n",
    "\n",
    "Suppose we have a dataset with features $(X)$ and target variable $(y)$. We start with an initial prediction $( \\hat{y}_0 )$, which could be the mean of \\( y \\).\n",
    "\n",
    "$[ \\hat{y}_0 = \\frac{1}{n} \\sum_{i=1}^{n} y_i ]$\n",
    "\n",
    "#### Step 2: Calculate Residuals\n",
    "\n",
    "Calculate the residuals (errors) for each instance in the dataset.\n",
    "\n",
    "$[ r_i = y_i - \\hat{y}_0 ]$\n",
    "\n",
    "#### Step 3: Train Weak Learner\n",
    "\n",
    "Train a decision tree on the residuals $( r )$. The decision tree will learn to predict the residuals.\n",
    "\n",
    "#### Step 4: Update Predictions\n",
    "\n",
    "Update the predictions by adding the predictions from the decision tree to the initial predictions.\n",
    "\n",
    "$[ \\hat{y}_1 = \\hat{y}_0 + \\text{tree}_1(X) ]$\n",
    "\n",
    "#### Step 5: Repeat\n",
    "\n",
    "Repeat steps 2-4 for a specified number of iterations. Each iteration adds another decision tree that corrects the errors of the previous ensemble.\n",
    "\n",
    "### Gradient Boosting Parameters\n",
    "\n",
    "1. **Number of Trees (n_estimators)**: The number of decision trees to be added. More trees can improve accuracy but also increase computation time and risk overfitting.\n",
    "\n",
    "2. **Learning Rate**: A scaling factor applied to each tree's contribution. Lower values make the model more robust to overfitting but require more trees.\n",
    "\n",
    "3. **Tree Depth**: Controls the complexity of each decision tree. Deeper trees can capture more information but also risk overfitting.\n",
    "\n",
    "### Popular Gradient Boosting Libraries\n",
    "\n",
    "1. **XGBoost**: An optimized gradient boosting library that is efficient and highly scalable.\n",
    "2. **LightGBM**: A gradient boosting framework that uses tree-based learning algorithms, designed to be efficient and fast.\n",
    "3. **CatBoost**: A gradient boosting library that handles categorical features well and is known for its ease of use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GradientBoost Algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "california  = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns = california.feature_names)#independent columns\n",
    "#Dependent column\n",
    "y = pd.Series(california.target)#Median value of house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing Data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3.585\n",
       "2    3.521\n",
       "3    3.413\n",
       "4    3.422\n",
       "5    2.697\n",
       "6    2.992\n",
       "7    2.414\n",
       "8    2.267\n",
       "9    2.611\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Dataset, Test and Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have 80% training set and 20% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gradientboost regressor object\n",
    "gradientregressor = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we have two important parameters:\n",
    " - n_estimators: Number of weak learners to train iteratively \n",
    " - learning_rate = It contributes to the weights of weak learners. It uses `1` as a default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training gradient boost regressor\n",
    "model = gradientregressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
