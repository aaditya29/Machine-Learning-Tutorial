{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is AdaBoost?\n",
    "\n",
    "**AdaBoost**, short for Adaptive Boosting, is a machine learning algorithm that combines multiple weak classifiers to form a strong classifier. A weak classifier is one that performs slightly better than random guessing, while a strong classifier is one that performs very well. AdaBoost is particularly good at improving the performance of these weak classifiers.\n",
    "\n",
    "### How does AdaBoost work?\n",
    "\n",
    "1. **Initialize Weights**: Each training example is assigned a weight. Initially, all weights are set equally.\n",
    "2. **Train Weak Classifier**: A weak classifier is trained using the weighted training data.\n",
    "3. **Compute Weak Classifier Error**: The error of the weak classifier is calculated based on the weights of the misclassified examples.\n",
    "4. **Update Weights**: The weights of the misclassified examples are increased, while the weights of correctly classified examples are decreased. This way, the algorithm focuses more on the difficult examples in subsequent iterations.\n",
    "5. **Combine Weak Classifiers**: The weak classifiers are combined to form a strong classifier. Each weak classifier is assigned a weight based on its accuracy.\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "We'll use the `scikit-learn` library in Python to implement AdaBoost. `scikit-learn` provides a convenient way to use AdaBoost with its `AdaBoostClassifier` class.\n",
    "\n",
    "1. **Install scikit-learn**: If you haven't installed it yet, you can do so using pip:\n",
    "   ```bash\n",
    "   pip install scikit-learn\n",
    "   ```\n",
    "\n",
    "2. **Import Libraries**: Start by importing the necessary libraries.\n",
    "   ```python\n",
    "   from sklearn.ensemble import AdaBoostClassifier\n",
    "   from sklearn.tree import DecisionTreeClassifier\n",
    "   from sklearn.datasets import make_classification\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import accuracy_score\n",
    "   ```\n",
    "\n",
    "3. **Create Dataset**: For simplicity, we'll create a synthetic dataset.\n",
    "   ```python\n",
    "   # Create synthetic dataset\n",
    "   X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "   \n",
    "   # Split the dataset into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "4. **Train AdaBoost Classifier**: We'll use a decision tree as the weak classifier.\n",
    "   ```python\n",
    "   # Initialize the base classifier\n",
    "   base_clf = DecisionTreeClassifier(max_depth=1)\n",
    "   \n",
    "   # Initialize the AdaBoost classifier\n",
    "   ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "   \n",
    "   # Train the AdaBoost classifier\n",
    "   ada_clf.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
    "   ```python\n",
    "   # Make predictions on the test set\n",
    "   y_pred = ada_clf.predict(X_test)\n",
    "   \n",
    "   # Calculate accuracy\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "   ```\n",
    "\n",
    "### Key Parameters of AdaBoost\n",
    "\n",
    "- `base_estimator`: The weak classifier used in AdaBoost. Here, we use `DecisionTreeClassifier`.\n",
    "- `n_estimators`: The number of weak classifiers to train.\n",
    "- `learning_rate`: Shrinks the contribution of each weak classifier. There is a trade-off between `learning_rate` and `n_estimators`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Concepts Behind AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost involves several key mathematical concepts:\n",
    "1. **Weights**: Weights are assigned to each training example to indicate their importance.\n",
    "2. **Weak Classifiers**: Each weak classifier focuses on the weighted training data.\n",
    "3. **Error Calculation**: The error of each weak classifier is calculated.\n",
    "4. **Alpha Calculation**: Alpha values are calculated to determine the importance of each weak classifier.\n",
    "5. **Weight Update**: Weights are updated based on the classifierâ€™s performance.\n",
    "6. **Final Strong Classifier**: The final strong classifier is a weighted sum of the weak classifiers.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "#### 1. Initialize Weights\n",
    "\n",
    "Each training example is assigned an initial weight $( w_i )$. For $( N )$ training examples, the initial weights are:\n",
    "\n",
    "$[ w_i = \\frac{1}{N} ]$\n",
    "\n",
    "#### 2. Train Weak Classifier\n",
    "\n",
    "A weak classifier $( h_t(x) )$ is trained on the weighted training data.\n",
    "\n",
    "#### 3. Compute Weak Classifier Error\n",
    "\n",
    "The error $( \\epsilon_t )$ of the weak classifier is calculated as:\n",
    "\n",
    "$[ \\epsilon_t = \\sum_{i=1}^N w_i \\cdot I(y_i \\neq h_t(x_i)) ]$\n",
    "\n",
    "where $( I(\\cdot) )$ is the indicator function, which is 1 if $( y_i \\neq h_t(x_i) )$ and 0 otherwise.\n",
    "\n",
    "#### 4. Compute Alpha\n",
    "\n",
    "Alpha $( \\alpha_t )$ is a measure of the importance of the weak classifier:\n",
    "\n",
    "$[ \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) ]$\n",
    "\n",
    "#### 5. Update Weights\n",
    "\n",
    "The weights are updated to give more importance to misclassified examples:\n",
    "\n",
    "$[ w_i \\leftarrow w_i \\cdot \\exp(\\alpha_t \\cdot I(y_i \\neq h_t(x_i))) ]$\n",
    "\n",
    "The weights are then normalized:\n",
    "\n",
    "$[ w_i \\leftarrow \\frac{w_i}{\\sum_{j=1}^N w_j} ]$\n",
    "\n",
    "#### 6. Final Strong Classifier\n",
    "\n",
    "The final strong classifier $( H(x) )$ is a weighted sum of the weak classifiers:\n",
    "\n",
    "$[ H(x) = \\text{sign} \\left( \\sum_{t=1}^T \\alpha_t h_t(x) \\right) ]$\n",
    "\n",
    "where $( T )$ is the total number of weak classifiers.\n",
    "\n",
    "### Example with Math\n",
    "\n",
    "Let's go through a simple example with three training examples and two weak classifiers.\n",
    "\n",
    "**Training Data:**\n",
    "- $( (x_1, y_1) = (1, 1) )$\n",
    "- $( (x_2, y_2) = (2, -1) )$\n",
    "- $( (x_3, y_3) = (3, 1) )$\n",
    "\n",
    "**Initialize Weights:**\n",
    "$[ w_1 = w_2 = w_3 = \\frac{1}{3} ]$\n",
    "\n",
    "**First Weak Classifier $( h_1(x) )$:**\n",
    "- $( h_1(1) = 1 )$\n",
    "- $( h_1(2) = 1 )$\n",
    "- $( h_1(3) = -1 )$\n",
    "\n",
    "**Error Calculation:**\n",
    "$[ \\epsilon_1 = \\frac{1}{3} (w_2 + w_3) = \\frac{2}{3} ]$\n",
    "\n",
    "**Alpha Calculation:**\n",
    "$[ \\alpha_1 = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_1}{\\epsilon_1} \\right) = \\frac{1}{2} \\ln \\left( \\frac{1/3}{2/3} \\right) = \\frac{1}{2} \\ln \\left( \\frac{1}{2} \\right) = -\\frac{1}{2} \\ln(2) ]$\n",
    "\n",
    "**Update Weights:**\n",
    "$[ w_1 \\leftarrow w_1 \\cdot \\exp(-\\frac{1}{2} \\ln(2) \\cdot 0) = \\frac{1}{3} ]$\n",
    "$[ w_2 \\leftarrow w_2 \\cdot \\exp(-\\frac{1}{2} \\ln(2) \\cdot 1) = \\frac{1}{3} \\cdot \\frac{1}{\\sqrt{2}} = \\frac{1}{3\\sqrt{2}} ]$\n",
    "$[ w_3 \\leftarrow w_3 \\cdot \\exp(-\\frac{1}{2} \\ln(2) \\cdot 1) = \\frac{1}{3\\sqrt{2}} ]$\n",
    "\n",
    "Normalize:\n",
    "$[ Z = \\frac{1}{3} + \\frac{1}{3\\sqrt{2}} + \\frac{1}{3\\sqrt{2}} = \\frac{1}{3} + \\frac{2}{3\\sqrt{2}} ]$\n",
    "$[ w_1 \\leftarrow \\frac{w_1}{Z} ]$\n",
    "$[ w_2 \\leftarrow \\frac{w_2}{Z} ]$\n",
    "$[ w_3 \\leftarrow \\frac{w_3}{Z} ]$\n",
    "\n",
    "\n",
    "> Repeat this process for subsequent weak classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
