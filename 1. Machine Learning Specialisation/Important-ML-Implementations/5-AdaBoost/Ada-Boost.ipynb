{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is AdaBoost?\n",
    "\n",
    "**AdaBoost**, short for Adaptive Boosting, is a machine learning algorithm that combines multiple weak classifiers to form a strong classifier. A weak classifier is one that performs slightly better than random guessing, while a strong classifier is one that performs very well. AdaBoost is particularly good at improving the performance of these weak classifiers.\n",
    "\n",
    "### How does AdaBoost work?\n",
    "\n",
    "1. **Initialize Weights**: Each training example is assigned a weight. Initially, all weights are set equally.\n",
    "2. **Train Weak Classifier**: A weak classifier is trained using the weighted training data.\n",
    "3. **Compute Weak Classifier Error**: The error of the weak classifier is calculated based on the weights of the misclassified examples.\n",
    "4. **Update Weights**: The weights of the misclassified examples are increased, while the weights of correctly classified examples are decreased. This way, the algorithm focuses more on the difficult examples in subsequent iterations.\n",
    "5. **Combine Weak Classifiers**: The weak classifiers are combined to form a strong classifier. Each weak classifier is assigned a weight based on its accuracy.\n",
    "\n",
    "### Step-by-Step Python Implementation\n",
    "\n",
    "We'll use the `scikit-learn` library in Python to implement AdaBoost. `scikit-learn` provides a convenient way to use AdaBoost with its `AdaBoostClassifier` class.\n",
    "\n",
    "1. **Install scikit-learn**: If you haven't installed it yet, you can do so using pip:\n",
    "   ```bash\n",
    "   pip install scikit-learn\n",
    "   ```\n",
    "\n",
    "2. **Import Libraries**: Start by importing the necessary libraries.\n",
    "   ```python\n",
    "   from sklearn.ensemble import AdaBoostClassifier\n",
    "   from sklearn.tree import DecisionTreeClassifier\n",
    "   from sklearn.datasets import make_classification\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import accuracy_score\n",
    "   ```\n",
    "\n",
    "3. **Create Dataset**: For simplicity, we'll create a synthetic dataset.\n",
    "   ```python\n",
    "   # Create synthetic dataset\n",
    "   X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "   \n",
    "   # Split the dataset into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "4. **Train AdaBoost Classifier**: We'll use a decision tree as the weak classifier.\n",
    "   ```python\n",
    "   # Initialize the base classifier\n",
    "   base_clf = DecisionTreeClassifier(max_depth=1)\n",
    "   \n",
    "   # Initialize the AdaBoost classifier\n",
    "   ada_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "   \n",
    "   # Train the AdaBoost classifier\n",
    "   ada_clf.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Make Predictions**: Use the trained model to make predictions on the test set.\n",
    "   ```python\n",
    "   # Make predictions on the test set\n",
    "   y_pred = ada_clf.predict(X_test)\n",
    "   \n",
    "   # Calculate accuracy\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "   ```\n",
    "\n",
    "### Key Parameters of AdaBoost\n",
    "\n",
    "- `base_estimator`: The weak classifier used in AdaBoost. Here, we use `DecisionTreeClassifier`.\n",
    "- `n_estimators`: The number of weak classifiers to train.\n",
    "- `learning_rate`: Shrinks the contribution of each weak classifier. There is a trade-off between `learning_rate` and `n_estimators`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
