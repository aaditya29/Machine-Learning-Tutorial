{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d3a6bd",
   "metadata": {},
   "source": [
    "# Softmax Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9d73c",
   "metadata": {},
   "source": [
    "## 1. **Intuitive Overview**\n",
    "\n",
    "Softmax Regression, also known as **Multinomial Logistic Regression**, is a generalization of logistic regression for multi-class classification problems. While logistic regression is used for binary classification, softmax regression can handle cases where the target variable can take on more than two classes.\n",
    "\n",
    "**Analogy:**  \n",
    "Imagine you are at an ice cream shop with three flavors: vanilla, chocolate, and strawberry. Given your preferences (features), softmax regression helps estimate the probability that you will choose each flavor.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Mathematical Foundations**\n",
    "\n",
    "### **2.1. Model Formulation**\n",
    "\n",
    "Given an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ and $K$ possible classes, softmax regression models the probability that $\\mathbf{x}$ belongs to class $k$ as:\n",
    "\n",
    "$[\n",
    "P(y = k \\mid \\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_k^\\top \\mathbf{x} + b_k)}{\\sum_{j=1}^K \\exp(\\mathbf{w}_j^\\top \\mathbf{x} + b_j)}\n",
    "]$\n",
    "\n",
    "- $\\mathbf{w}_k$ is the weight vector for class $k$\n",
    "- $b_k$ is the bias for class $k$\n",
    "\n",
    "The denominator ensures that the probabilities sum to 1 across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2. The Softmax Function**\n",
    "\n",
    "The **softmax function** transforms a vector of real numbers into a probability distribution:\n",
    "\n",
    "$[\n",
    "\\text{softmax}(\\mathbf{z})_k = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}\n",
    "]$\n",
    "\n",
    "where $\\mathbf{z} = [z_1, z_2, ..., z_K]$.\n",
    "\n",
    "**Key Properties:**\n",
    "- All outputs are in $(0, 1)$\n",
    "- Outputs sum to 1\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3. Loss Function: Cross-Entropy**\n",
    "\n",
    "For a dataset with $N$ samples, the **cross-entropy loss** is:\n",
    "\n",
    "$[\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{ik} \\log P(y = k \\mid \\mathbf{x}_i)\n",
    "]$\n",
    "\n",
    "where $y_{ik}$ is 1 if sample $i$ belongs to class $k$, else 0.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.4. Gradient Derivation**\n",
    "\n",
    "Letâ€™s derive the gradient for parameter $\\mathbf{w}_k$:\n",
    "\n",
    "$[\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}_k} = -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_{ik} - P(y = k \\mid \\mathbf{x}_i) \\right] \\mathbf{x}_i\n",
    "]$\n",
    "\n",
    "**Proof Sketch:**\n",
    "- The loss is differentiable and convex.\n",
    "- The gradient points in the direction to adjust weights to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Why Softmax?**\n",
    "\n",
    "- **Generalizes logistic regression** to multi-class problems.\n",
    "- **Probabilistic interpretation:** Outputs can be interpreted as class probabilities.\n",
    "- **Differentiable:** Suitable for gradient-based optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Real-World Analogy**\n",
    "\n",
    "Think of softmax as a \"voting system\" where each class assigns a score to the input. The softmax function converts these scores into probabilities, much like how a group of friends might each rate a restaurant, and you use their ratings to decide where to eat.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Modern Applications in AI/ML**\n",
    "\n",
    "- **Image Classification:** Assigning labels to images (e.g., cats, dogs, cars).\n",
    "- **Natural Language Processing:** Predicting the next word in a sentence (language modeling).\n",
    "- **Recommender Systems:** Ranking items for users.\n",
    "- **Neural Networks:** The final layer of many neural networks for classification uses softmax.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Summary Table**\n",
    "\n",
    "| Aspect                | Logistic Regression (Binary) | Softmax Regression (Multi-class) |\n",
    "|-----------------------|------------------------------|-----------------------------------|\n",
    "| Output                | Probability (2 classes)      | Probability distribution (K classes) |\n",
    "| Activation Function   | Sigmoid                      | Softmax                           |\n",
    "| Loss Function         | Binary Cross-Entropy         | Categorical Cross-Entropy         |\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "Softmax regression is a foundational tool for multi-class classification, combining clear probabilistic interpretation, mathematical elegance, and practical utility in modern AI systems. Understanding its derivation and intuition is essential for mastering machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55ea3b",
   "metadata": {},
   "source": [
    "## Implementing From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6b6cb",
   "metadata": {},
   "source": [
    "We will perform following steps while implementing softmax regression from scratch:\n",
    "1. Generate synthetic data for 3 classes\n",
    "2. Build softmax function\n",
    "3. Compute cross-entropy loss\n",
    "4. Calculate gradients\n",
    "5. Train with gradient descent\n",
    "6. Visualize accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68840f24",
   "metadata": {},
   "source": [
    "### **1. Generating Synthetic Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7e0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bb7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
