{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6e51bd",
   "metadata": {},
   "source": [
    "# Implementing Back-Propagation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0d0c1",
   "metadata": {},
   "source": [
    "Backpropagation is the central algorithm for training neural networks. It allows us to efficiently compute gradients of loss functions with respect to all weights in the network. These gradients are then used to update the weights via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b2c08",
   "metadata": {},
   "source": [
    "\n",
    "## **1. Motivation: Why Backpropagation?**\n",
    "\n",
    "Let’s start with the key challenge in training a neural network:\n",
    "\n",
    "> **We want to adjust the weights in a neural network so that it makes better predictions.**\n",
    "\n",
    "But to adjust the weights, we need to compute **how the loss changes with respect to each weight**, i.e., compute **gradients**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Naively doing this is **computationally expensive**, especially when we have millions of weights and layers.\n",
    "Backpropagation is an efficient way to **reuse intermediate computations**, reducing the cost of computing all the gradients from exponential to linear in the number of layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341941a",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Forward Pass: Flow of Information**\n",
    "\n",
    "Think of the neural network as a **factory assembly line**:\n",
    "\n",
    "* Inputs go in.\n",
    "* Each layer does a transformation.\n",
    "* The final output is a prediction.\n",
    "* The loss measures how wrong the prediction was.\n",
    "\n",
    "For example, in a 2-layer neural network:\n",
    "\n",
    "```plaintext\n",
    "Input x → [W1, b1] → h1 = f1(W1x + b1)\n",
    "           ↓\n",
    "         [W2, b2] → ŷ = f2(W2h1 + b2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179fb37",
   "metadata": {},
   "source": [
    "## **3. Backward Pass: Flow of Responsibility**\n",
    "\n",
    "Now we ask: *Which weight caused how much of the error?*\n",
    "\n",
    "Backpropagation works **backwards**, starting from the loss:\n",
    "\n",
    "* How much did each output neuron contribute to the loss?\n",
    "* How much did each hidden neuron contribute to the output?\n",
    "* How much did each weight contribute to its neuron’s output?\n",
    "\n",
    "This is where **the chain rule of calculus** comes in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889778b8",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Mathematical Foundation: The Chain Rule**\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "* $z = f(y)$\n",
    "* $y = g(x)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "This rule allows us to “chain” the effects of input on the output. In neural networks, the output is a function of a function of a function… So we use the chain rule **repeatedly**, layer by layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: One Hidden Layer**\n",
    "\n",
    "Let:\n",
    "\n",
    "* $a = Wx + b$ — pre-activation\n",
    "* $h = \\sigma(a)$ — activation (e.g., ReLU or sigmoid)\n",
    "* $\\hat{y} = Uh + c$ — final score\n",
    "* $\\mathcal{L} = \\text{loss}(\\hat{y}, y)$\n",
    "\n",
    "We want:\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W}$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "\n",
    "Let’s compute:\n",
    "\n",
    "1. $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}}$ → from the loss function (e.g., softmax + cross entropy)\n",
    "2. $\\frac{\\partial \\hat{y}}{\\partial h} = U$\n",
    "3. $\\frac{\\partial h}{\\partial a} = \\sigma'(a)$\n",
    "4. $\\frac{\\partial a}{\\partial W} = x$\n",
    "\n",
    "Chain them:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial a} \\cdot \\frac{\\partial a}{\\partial W}\n",
    "$$\n",
    "\n",
    "In practice, we implement this using matrix derivatives and vectorized operations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de009b5d",
   "metadata": {},
   "source": [
    "## **5. Real-World Analogy**\n",
    "\n",
    "Think of a restaurant kitchen with multiple stations:\n",
    "\n",
    "* Ingredients (input)\n",
    "* Chopping station (layer 1)\n",
    "* Cooking station (layer 2)\n",
    "* Plating station (layer 3)\n",
    "* Final dish (output)\n",
    "\n",
    "Now suppose a dish was terrible (high loss). To improve:\n",
    "\n",
    "* The chef at the plating station adjusts based on feedback.\n",
    "* That feedback goes backward to the cook.\n",
    "* The cook blames the chopping for bad ingredients.\n",
    "* Each station gets **blame** proportionally to its **influence**.\n",
    "\n",
    "This blame assignment is the essence of backpropagation.\n",
    "\n",
    "\n",
    "## **6. Vectorized Implementation**\n",
    "\n",
    "For efficiency, we use matrix notation and perform the entire backward pass in **vectorized form**.\n",
    "\n",
    "Let’s say for one hidden layer:\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "z1 = X @ W1 + b1\n",
    "a1 = relu(z1)\n",
    "z2 = a1 @ W2 + b2\n",
    "y_hat = softmax(z2)\n",
    "loss = cross_entropy(y_hat, y)\n",
    "```\n",
    "\n",
    "Then backpropagation:\n",
    "\n",
    "```python\n",
    "# Backward pass\n",
    "dz2 = y_hat - y_true       # gradient from loss to z2\n",
    "dW2 = a1.T @ dz2\n",
    "db2 = dz2.sum(axis=0)\n",
    "\n",
    "da1 = dz2 @ W2.T\n",
    "dz1 = da1 * relu_grad(z1)  # gradient from activation\n",
    "dW1 = X.T @ dz1\n",
    "db1 = dz1.sum(axis=0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfbd438",
   "metadata": {},
   "source": [
    "## **7. Why Backprop is Efficient**\n",
    "\n",
    "Naively computing each gradient would be **costly**, especially with multiple layers and neurons. Backprop takes advantage of **dynamic programming** by:\n",
    "\n",
    "* Storing intermediate values during forward pass\n",
    "* Reusing them in backward pass (no recomputation)\n",
    "* Propagating gradients in a chain (chain rule)\n",
    "\n",
    "This reduces computation to **O(L)**, where L = number of layers.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Modern Applications of Backpropagation**\n",
    "\n",
    "Backpropagation is the backbone of modern deep learning. Some use cases:\n",
    "\n",
    "* **Vision**: CNNs use backprop to learn filters for edge detection, objects, etc.\n",
    "* **Language**: Transformers use backprop to learn contextual embeddings.\n",
    "* **Reinforcement Learning**: Policy gradients and Q-learning rely on backprop.\n",
    "* **Biology**: Neural networks trained with backprop predict protein structures (AlphaFold).\n",
    "* **Art**: Style transfer, GANs, and diffusion models are all powered by backprop.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Limitations and Challenges**\n",
    "\n",
    "* **Vanishing gradients**: In deep networks, early layers get very small gradients.\n",
    "* **Exploding gradients**: Opposite problem; gradients grow too large.\n",
    "* **Non-convexity**: Many local minima or saddle points.\n",
    "* **Computational cost**: Training large models can require GPUs/TPUs and a lot of energy.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Backprop vs. Alternatives**\n",
    "\n",
    "While backprop dominates deep learning, some are exploring:\n",
    "\n",
    "* **Hebbian Learning**: Inspired by neuroscience.\n",
    "* **Feedback Alignment**: Replaces exact weight transposes with random matrices.\n",
    "* **Neuro-symbolic methods**: Mix logic with learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bb928",
   "metadata": {},
   "source": [
    "## Implementing From the Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2d378",
   "metadata": {},
   "source": [
    "We will build a simple neural network with:\n",
    "- Input layer: 2 features (e.g., X shape = (N, 2))\n",
    "- Hidden layer: 3 neurons with ReLU\n",
    "- Output layer: 2 classes with Softmax\n",
    "- Loss: Cross-entropy\n",
    "\n",
    "> Goal: Classify input points into 2 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6db3a2",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Pass: Understanding the Math\n",
    "\n",
    "Let’s define:\n",
    "\n",
    "* $X \\in \\mathbb{R}^{N \\times D}$: input data\n",
    "* $y \\in \\{0,1\\}$: true labels\n",
    "* $W_1 \\in \\mathbb{R}^{D \\times H}$, $b_1 \\in \\mathbb{R}^{H}$: weights/biases for hidden layer\n",
    "* $W_2 \\in \\mathbb{R}^{H \\times C}$, $b_2 \\in \\mathbb{R}^{C}$: weights/biases for output layer\n",
    "\n",
    "#### Forward equations:\n",
    "\n",
    "1. **Hidden layer pre-activation**:\n",
    "\n",
    "   $$\n",
    "   z_1 = X W_1 + b_1\n",
    "   $$\n",
    "2. **Hidden activation (ReLU)**:\n",
    "\n",
    "   $$\n",
    "   a_1 = \\text{ReLU}(z_1)\n",
    "   $$\n",
    "3. **Output logits**:\n",
    "\n",
    "   $$\n",
    "   z_2 = a_1 W_2 + b_2\n",
    "   $$\n",
    "4. **Softmax probabilities**:\n",
    "\n",
    "   $$\n",
    "   \\hat{y} = \\text{softmax}(z_2)\n",
    "   $$\n",
    "\n",
    "#### Loss (cross-entropy for classification):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{y}_{i, y_i})\n",
    "$$\n",
    "\n",
    "### Backpropagation: Chain Rule Step by Step\n",
    "\n",
    "We want gradients of loss w\\.r.t all weights:\n",
    "\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W_2}$, $\\frac{\\partial \\mathcal{L}}{\\partial b_2}$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial W_1}$, $\\frac{\\partial \\mathcal{L}}{\\partial b_1}$\n",
    "\n",
    "#### Step-by-step:\n",
    "\n",
    "1. **Loss to logits**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial z_2} = \\hat{y} - y_{\\text{one-hot}}\n",
    "   $$\n",
    "2. **Logits to output weights**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W_2} = a_1^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z_2}\n",
    "   $$\n",
    "3. **Output back to hidden**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial a_1} = \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\cdot W_2^T\n",
    "   $$\n",
    "4. **Apply ReLU derivative**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial z_1} = \\frac{\\partial \\mathcal{L}}{\\partial a_1} \\cdot \\text{ReLU}'(z_1)\n",
    "   $$\n",
    "5. **Input to hidden weights**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W_1} = X^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial z_1}\n",
    "   $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64837c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58b466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU activation function and its derivative\n",
    "def relu(x):\n",
    "    \"\"\"Rectified Linear Unit activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x>0).astype(float) # Derivative of ReLU is 1 for x > 0, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d56ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax function\n",
    "def softmax(logits):\n",
    "    logits -= np.max(logits, axis=-1, keepdims=True)  # For numerical stability\n",
    "    exps = np.exp(logits) # Avoid overflow by subtracting max\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)# Normalize to get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da95fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-entropy loss function\n",
    "def cross_entropy_loss(probs, y_true):\n",
    "    N = y_true.shape[0]# Number of samples\n",
    "    correct_logprobs = -np.log(probs[np.arange(N), y_true] + 1e-15)  # Add small constant for numerical stability\n",
    "    return np.sum(correct_logprobs) / N  # Average loss over all samples\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot(y, num_classes):\n",
    "    \"\"\"Convert class labels to one-hot encoded format.\"\"\"\n",
    "    return np.eye(num_classes)[y]  # Create identity matrix and index with y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a0068",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18884ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, hidden_dim=3, lr=1.0, num_epochs=1000):\n",
    "    N, D = X.shape  # number of samples and input dimension\n",
    "    C = np.max(y) + 1  # number of classes (assuming y contains class indices starting from 0)\n",
    "\n",
    "    #initialising weights and biases\n",
    "    W1  = 0.01 * np.random.randn(D, hidden_dim)  # Input to hidden layer weights\n",
    "    b1  = np.zeros((1, hidden_dim))  # Hidden layer biases\n",
    "    W2  = 0.01 * np.random.randn(hidden_dim, C)  # Hidden to output layer weights\n",
    "    b2  = np.zeros((1, C))  # Output layer biases\n",
    "    y_onehot = one_hot(y, C)  # Converting labels to one-hot encoding\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        # Forward pass\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        probs = softmax(z2)\n",
    "        loss = cross_entropy_loss(probs, y)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "        # Backward pass\n",
    "        dz2 = probs - y_onehot  # (N, C)\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ W2.T\n",
    "        dz1 = da1 * relu_derivative(z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58789b78",
   "metadata": {},
   "source": [
    "### Testing on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a06a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866610a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.6932\n",
      "Epoch 100: Loss = 17.2694\n",
      "Epoch 200: Loss = 17.2694\n",
      "Epoch 300: Loss = 17.2694\n",
      "Epoch 400: Loss = 17.2694\n",
      "Epoch 500: Loss = 17.2694\n",
      "Epoch 600: Loss = 17.2694\n",
      "Epoch 700: Loss = 17.2694\n",
      "Epoch 800: Loss = 17.2694\n",
      "Epoch 900: Loss = 17.2694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-2.59549192e+02, -9.40066550e+03, -1.39900159e+28],\n",
       "        [-2.75464751e+01,  1.17985290e+02, -3.37004499e+27]]),\n",
       " array([[-1.12887767e+03, -3.66176760e+04, -2.97129997e+28]]),\n",
       " array([[-1.28460763e+02,  1.28458781e+02],\n",
       "        [-3.19103476e+03,  3.19106043e+03],\n",
       "        [-4.44034525e+29,  4.44034525e+29]]),\n",
       " array([[-46.46629019,  46.46629019]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating synthetic data\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "#training the network\n",
    "train(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
