{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6e51bd",
   "metadata": {},
   "source": [
    "# Implementing Back-Propagation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0d0c1",
   "metadata": {},
   "source": [
    "Backpropagation is the central algorithm for training neural networks. It allows us to efficiently compute gradients of loss functions with respect to all weights in the network. These gradients are then used to update the weights via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b2c08",
   "metadata": {},
   "source": [
    "\n",
    "## **1. Motivation: Why Backpropagation?**\n",
    "\n",
    "Let’s start with the key challenge in training a neural network:\n",
    "\n",
    "> **We want to adjust the weights in a neural network so that it makes better predictions.**\n",
    "\n",
    "But to adjust the weights, we need to compute **how the loss changes with respect to each weight**, i.e., compute **gradients**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Naively doing this is **computationally expensive**, especially when we have millions of weights and layers.\n",
    "Backpropagation is an efficient way to **reuse intermediate computations**, reducing the cost of computing all the gradients from exponential to linear in the number of layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341941a",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Forward Pass: Flow of Information**\n",
    "\n",
    "Think of the neural network as a **factory assembly line**:\n",
    "\n",
    "* Inputs go in.\n",
    "* Each layer does a transformation.\n",
    "* The final output is a prediction.\n",
    "* The loss measures how wrong the prediction was.\n",
    "\n",
    "For example, in a 2-layer neural network:\n",
    "\n",
    "```plaintext\n",
    "Input x → [W1, b1] → h1 = f1(W1x + b1)\n",
    "           ↓\n",
    "         [W2, b2] → ŷ = f2(W2h1 + b2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179fb37",
   "metadata": {},
   "source": [
    "## **3. Backward Pass: Flow of Responsibility**\n",
    "\n",
    "Now we ask: *Which weight caused how much of the error?*\n",
    "\n",
    "Backpropagation works **backwards**, starting from the loss:\n",
    "\n",
    "* How much did each output neuron contribute to the loss?\n",
    "* How much did each hidden neuron contribute to the output?\n",
    "* How much did each weight contribute to its neuron’s output?\n",
    "\n",
    "This is where **the chain rule of calculus** comes in.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889778b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
