{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8525a7",
   "metadata": {},
   "source": [
    "# Creating Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1305a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67944",
   "metadata": {},
   "source": [
    "### 1.1 Importing PDF Document for our Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict# for type hints\n",
    "from spacy.lang.en import English\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb454ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf. Proceeding to read the file.\n"
     ]
    }
   ],
   "source": [
    "# getting pdf documents from local system\n",
    "pdf_path = \"/Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf\"\n",
    "\n",
    "# download pdf if not present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Given path {pdf_path} does not exist. Downloading the pdf file!!!\")\n",
    "    url = \"https://www.cs.mcgill.ca/~akroit/math/compsci/Cormen%20Introduction%20to%20Algorithms.pdf\"\n",
    "    \n",
    "    filename = pdf_path\n",
    "    response = requests.get(url)# download the file\n",
    "    \n",
    "    if response.status_code == 200: # check if the download was successful\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)# save the file\n",
    "        print(f\"File downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists at {pdf_path}. Proceeding to read the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866fa3",
   "metadata": {},
   "source": [
    "Sine now we have imported our file now next step is to preprocess the text as we read it. We have imported the pages of book in the `file_path` and now we can open and read it with `PyMuPDF` by typing command `import fitz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1641a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing text formatting according to our need for clrs pdf\n",
    "\n",
    "def text_formatter(text:str):\n",
    "    \n",
    "    #fixing hyphenated words\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    #fixing indentation issues in paragraphs\n",
    "    lines = text.split('\\n')# split text into lines\n",
    "    processed_lines = []# list to hold processed lines\n",
    "    \n",
    "    for line in lines:\n",
    "        if re.match(r'^\\s{4,}', line) or line.strip().startswith('//'):\n",
    "            processed_lines.append('__CODE__' + line)\n",
    "        else:\n",
    "            processed_lines.append(line)\n",
    "\n",
    "    text = '\\n'.join(processed_lines)# join lines back into text\n",
    "    \n",
    "    #removing excessive blank lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?<!__CODE__)  +', ' ', text)# collapse multiple spaces to single space except in code blocks\n",
    "    #emove __CODE__ markers but keep the indentation\n",
    "    text = text.replace('__CODE__', '')\n",
    "    # fixing spacing around punctuation except mathematical notation\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    # Preserving mathematical notation spacing\n",
    "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    # Remove excessive whitespace at start/end\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#to detext psuedo code blocks in the text of clrs pdf\n",
    "def is_algorithm_block(text: str) -> bool:\n",
    "    indicators = [\n",
    "        r'\\bif\\b.*\\bthen\\b',# if-then statements\n",
    "        r'\\bfor\\b.*\\bdo\\b',# for-do loops\n",
    "        r'\\bwhile\\b.*\\bdo\\b',# while-do loops\n",
    "        r'\\breturn\\b',# return statements\n",
    "        r'\\belse\\b',# else statements\n",
    "        r'\\bfunction\\b',# function definitions\n",
    "        r'\\bprocedure\\b',# procedure definitions\n",
    "        r'\\bbegin\\b',  # begin blocks\n",
    "        r'\\bend\\b',  # end blocks\n",
    "        r'^\\s*//.*',  # comments\n",
    "        r'[A-Z][A-Z-]+\\(',  # Function calls like sorting(), MERGE()\n",
    "        r'^\\s*[\\d]+\\.',  # Numbered steps\n",
    "    ]\n",
    "    \n",
    "    # check each pattern in text and return True if any matches\n",
    "    return any(re.search(pattern, text, re.IGNORECASE | re.MULTILINE) for pattern in indicators)\n",
    "\n",
    "# detecting section headers in the text of clrs pdf\n",
    "def is_section_header(text: str) -> bool:\n",
    "    #check if the text is in title case and not too long\n",
    "    text = text.strip()\n",
    "    return (len(text) < 100 and\n",
    "            len(text.split()) < 10 and\n",
    "            not text.endswith('.') and\n",
    "            len(text) > 0 and\n",
    "            not text[0].isdigit())\n",
    "\n",
    "\n",
    "# chunking texts for preserving context\n",
    "def smart_chunker(pages_and_texts: List[Dict],\n",
    "                  chunk_size: int = 1000,\n",
    "                  overlap: int = 200) -> List[Dict]:\n",
    "    chunks = []\n",
    "\n",
    "    for page_data in pages_and_texts:\n",
    "        text = page_data['text']\n",
    "        page_num = page_data['page_number']\n",
    "\n",
    "        # Split by double newlines to get paragraphs/blocks\n",
    "        blocks = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'has_algorithm': False,\n",
    "            'section_header': None\n",
    "        }\n",
    "\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "            if not block:\n",
    "                continue\n",
    "\n",
    "            # Check if this is a section header\n",
    "            if is_section_header(block):\n",
    "                # If we have a current chunk, save it\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                # Start new chunk with this header\n",
    "                current_chunk = block + \"\\n\\n\"\n",
    "                current_chunk_metadata = {\n",
    "                    'page_number': page_num,\n",
    "                    'has_algorithm': False,\n",
    "                    'section_header': block\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Check if this is an algorithm block\n",
    "            if is_algorithm_block(block):\n",
    "                current_chunk_metadata['has_algorithm'] = True\n",
    "\n",
    "                # If adding this would exceed chunk_size and we have content then save current chunk\n",
    "                if len(current_chunk) + len(block) > chunk_size and current_chunk:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap from previous\n",
    "                    overlap_text = current_chunk[-overlap:] if len(\n",
    "                        current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': True,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    # Add algorithm block to current chunk (keep it together!)\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "            else:\n",
    "                # Regular text block\n",
    "                if len(current_chunk) + len(block) > chunk_size and current_chunk:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_text = current_chunk[-overlap:] if len(\n",
    "                        current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': False,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "\n",
    "        # checking the last chunk from this page\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'page_number': current_chunk_metadata['page_number'],\n",
    "                'chunk_char_count': len(current_chunk),\n",
    "                'chunk_word_count': len(current_chunk.split()),\n",
    "                'chunk_token_count': len(current_chunk) / 4,\n",
    "                'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                'section_header': current_chunk_metadata['section_header']\n",
    "            })\n",
    "\n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = i\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)# opening doc\n",
    "    pages_and_texts = []# \n",
    "    for page_number, page in tqdm(enumerate(doc), desc=\"Reading PDF pages\"):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - 41,#adjusting to start of content for book\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split()),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text) / 4,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428a76be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f0e55a656042bb920ed6909f7e9d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 1313\n",
      "Total chunks: 4570\n",
      "Chunks with algorithms: 1859\n",
      "\n",
      "Sample chunk:\n",
      "{'text': 'vi\\nContents\\nII\\nSorting and Order Statistics\\nIntroduction', 'page_number': -35, 'chunk_char_count': 58, 'chunk_word_count': 8, 'chunk_token_count': 14.5, 'has_algorithm': False, 'section_header': 'vi\\nContents\\nII\\nSorting and Order Statistics\\nIntroduction', 'chunk_id': 10}\n"
     ]
    }
   ],
   "source": [
    "pages_and_texts = open_and_read_pdf(pdf_path)\n",
    "#creating smart chunks\n",
    "chunks = smart_chunker(pages_and_texts, chunk_size=1000, overlap=200)\n",
    "#results\n",
    "print(f\"Total pages: {len(pages_and_texts)}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Chunks with algorithms: {sum(1 for c in chunks if c['has_algorithm'])}\")\n",
    "# Looking at a sample chunk\n",
    "print(\"\\nSample chunk:\")\n",
    "print(chunks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aec235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
