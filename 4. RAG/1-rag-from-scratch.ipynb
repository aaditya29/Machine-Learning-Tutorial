{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8525a7",
   "metadata": {},
   "source": [
    "# Creating Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1305a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67944",
   "metadata": {},
   "source": [
    "### 1.1 Importing PDF Document for our Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict# for type hints\n",
    "from spacy.lang.en import English\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb454ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf. Proceeding to read the file.\n"
     ]
    }
   ],
   "source": [
    "# getting pdf documents from local system\n",
    "pdf_path = \"/Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf\"\n",
    "\n",
    "# download pdf if not present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Given path {pdf_path} does not exist. Downloading the pdf file!!!\")\n",
    "    url = \"https://www.cs.mcgill.ca/~akroit/math/compsci/Cormen%20Introduction%20to%20Algorithms.pdf\"\n",
    "    \n",
    "    filename = pdf_path\n",
    "    response = requests.get(url)# download the file\n",
    "    \n",
    "    if response.status_code == 200: # check if the download was successful\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)# save the file\n",
    "        print(f\"File downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists at {pdf_path}. Proceeding to read the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866fa3",
   "metadata": {},
   "source": [
    "Sine now we have imported our file now next step is to preprocess the text as we read it. We have imported the pages of book in the `file_path` and now we can open and read it with `PyMuPDF` by typing command `import fitz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6f61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    # Fixing hyphenated words split across lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    #Identify and preserve code blocks\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        #skiping empty lines\n",
    "        if not stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "\n",
    "        #checking if this looks like code or pseudocode\n",
    "        is_code_line = (\n",
    "            len(line) - len(line.lstrip()) >= 4 or  # 4+ space indent\n",
    "            stripped.startswith('//') or  # comment\n",
    "            re.match(r'^(if|for|while|return|else)\\b', stripped, re.IGNORECASE) or\n",
    "            re.match(r'^[A-Z][A-Z\\-]+\\(', stripped)  # FUNCTION-NAME(\n",
    "        )\n",
    "\n",
    "        if is_code_line:\n",
    "            in_code_block = True\n",
    "            processed_lines.append('__CODELINE__' + line)# checking codeline\n",
    "        else:\n",
    "            #checkingif we just exited a code block\n",
    "            if in_code_block:\n",
    "                processed_lines.append('__CODEEND__')\n",
    "                in_code_block = False\n",
    "            processed_lines.append(line)\n",
    "\n",
    "    text = '\\n'.join(processed_lines)\n",
    "\n",
    "    #paragraph joining for non code texts\n",
    "    text = re.sub(\n",
    "        r'(?<!__CODELINE__)(?<!__CODEEND__)(?<!\\n)\\n(?!__CODELINE__)(?!__CODEEND__)(?!\\n)(?![A-Z])', ' ', text)\n",
    "\n",
    "    #clearning up markers\n",
    "    text = text.replace('__CODELINE__', '')\n",
    "    text = text.replace('__CODEEND__', '\\n')\n",
    "\n",
    "    # removing excessive blank\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    #cllean up multiple spaces (but not at line start - that's indentation)\n",
    "    text = re.sub(r'([^\\n]) {2,}', r'\\1 ', text)\n",
    "\n",
    "    #spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "\n",
    "    # deleting standalone page numbers\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove common header/footer patterns\n",
    "    text = re.sub(r'^(Chapter|Section)\\s+\\d+.*$', '',\n",
    "                  text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_algorithm_block(text: str) -> bool:\n",
    "    indicators = [\n",
    "        bool(re.search(r'\\b(if|then|else)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\b(for|while|do)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\breturn\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'^\\s*//.*', text, re.MULTILINE)),  # comments\n",
    "        bool(re.search(r'[A-Z][A-Z\\-]+\\([^)]*\\)', text)),  # FUNCTION(...)\n",
    "        bool(re.search(r'A\\[\\s*\\d+', text)),  # Array notation A[1\n",
    "        bool(re.search(r'←|:=|=', text)),  # Assignment operators\n",
    "    ]\n",
    "    # Need at least 2 indicators and text should be important\n",
    "    return sum(indicators) >= 2 and len(text.split()) > 10\n",
    "\n",
    "\n",
    "#for detection of headers\n",
    "def is_section_header(text: str) -> bool:\n",
    "    text = text.strip()\n",
    "    if not text or len(text) > 100:\n",
    "        return False\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) > 15 or len(words) < 2:\n",
    "        return False\n",
    "\n",
    "    # Skip table of contents entries (have lots of dots)\n",
    "    if text.count('.') > 3:\n",
    "        return False\n",
    "\n",
    "    # Skip if it's just \"Contents\" or roman numerals\n",
    "    if text.lower() in ['contents', 'preface', 'index', 'references']:\n",
    "        return False\n",
    "\n",
    "    # Likely a header if it doesn't end with period and isn't too long\n",
    "    return not text.endswith('.')\n",
    "\n",
    "\n",
    "def is_toc_or_front_matter(text: str) -> bool:\n",
    "    \"\"\"Detect if text is from table of contents or front matter (should be skipped).\"\"\"\n",
    "    indicators = [\n",
    "        'contents' in text.lower()[:50],\n",
    "        'preface' in text.lower()[:50],\n",
    "        text.count('...') > 2,  # TOC dots\n",
    "        # Too many periods\n",
    "        text.count('.') > len(text.split()) * 0.5,\n",
    "        # Roman numerals only\n",
    "        bool(re.search(r'^[ivxlcdm]+$', text.strip(), re.IGNORECASE)),\n",
    "    ]\n",
    "    return any(indicators) or len(text.split()) < 5\n",
    "\n",
    "\n",
    "def smart_chunker(pages_and_texts: List[Dict],\n",
    "                  chunk_size: int = 1000,\n",
    "                  overlap: int = 200,\n",
    "                  skip_front_matter: bool = True) -> List[Dict]:\n",
    "    chunks = []\n",
    "\n",
    "    for page_data in pages_and_texts:\n",
    "        text = page_data['text']\n",
    "        page_num = page_data['page_number']\n",
    "\n",
    "        # Skip if this looks like front matter\n",
    "        if skip_front_matter and is_toc_or_front_matter(text):\n",
    "            continue\n",
    "\n",
    "        # Skip very short pages (likely artifacts)\n",
    "        if len(text.split()) < 10:\n",
    "            continue\n",
    "\n",
    "        # Split by double newlines to get paragraphs/blocks\n",
    "        blocks = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'has_algorithm': False,\n",
    "            'section_header': None\n",
    "        }\n",
    "\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "            if not block or len(block) < 10:\n",
    "                continue\n",
    "\n",
    "            # Check if this is a section header\n",
    "            if is_section_header(block):\n",
    "                # If we have a current chunk, save it\n",
    "                if current_chunk and len(current_chunk.split()) > 10:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                # Start new chunk with this header\n",
    "                current_chunk = block + \"\\n\\n\"\n",
    "                current_chunk_metadata = {\n",
    "                    'page_number': page_num,\n",
    "                    'has_algorithm': False,\n",
    "                    'section_header': block\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Check if this is an algorithm block\n",
    "            block_has_algo = is_algorithm_block(block)\n",
    "            if block_has_algo:\n",
    "                current_chunk_metadata['has_algorithm'] = True\n",
    "\n",
    "                # If adding this would exceed chunk_size significantly and we have content, save current chunk\n",
    "                if len(current_chunk) + len(block) > chunk_size * 1.5 and len(current_chunk.split()) > 20:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap from previous\n",
    "                    overlap_text = current_chunk[-overlap:] if len(\n",
    "                        current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': True,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    # Add algorithm block to current chunk (keep it together!)\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "            else:\n",
    "                # Regular text block\n",
    "                if len(current_chunk) + len(block) > chunk_size and len(current_chunk.split()) > 20:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_text = current_chunk[-overlap:] if len(\n",
    "                        current_chunk) > overlap else current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': False,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "\n",
    "        # Save the last chunk from this page if it's substantial\n",
    "        if current_chunk.strip() and len(current_chunk.split()) > 10:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'page_number': current_chunk_metadata['page_number'],\n",
    "                'chunk_char_count': len(current_chunk),\n",
    "                'chunk_word_count': len(current_chunk.split()),\n",
    "                'chunk_token_count': len(current_chunk) / 4,\n",
    "                'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                'section_header': current_chunk_metadata['section_header']\n",
    "            })\n",
    "\n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = i\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd593cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
