{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8525a7",
   "metadata": {},
   "source": [
    "# Creating Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1305a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67944",
   "metadata": {},
   "source": [
    "### 1.1 Importing PDF Document for our Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import random\n",
    "import requests\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict  # for type hints\n",
    "from spacy.lang.en import English\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d880ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb454ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf. Proceeding to read the file.\n"
     ]
    }
   ],
   "source": [
    "# getting pdf documents from local system\n",
    "pdf_path = \"/Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf\"\n",
    "\n",
    "# download pdf if not present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Given path {pdf_path} does not exist. Downloading the pdf file!!!\")\n",
    "    url = \"https://www.cs.mcgill.ca/~akroit/math/compsci/Cormen%20Introduction%20to%20Algorithms.pdf\"\n",
    "    \n",
    "    filename = pdf_path\n",
    "    response = requests.get(url)# download the file\n",
    "    \n",
    "    if response.status_code == 200: # check if the download was successful\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)# save the file\n",
    "        print(f\"File downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists at {pdf_path}. Proceeding to read the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866fa3",
   "metadata": {},
   "source": [
    "Sine now we have imported our file now next step is to preprocess the text as we read it. We have imported the pages of book in the `file_path` and now we can open and read it with `PyMuPDF` by typing command `import fitz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0082351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    #fixing hyphenated words split across lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # fixing words split across lines WITHOUT hyphens\n",
    "    text = re.sub(r'(\\w+)\\s*\\n\\s*(\\w+)', lambda m: m.group(1) + m.group(2) if m.group(2)[0].islower() else m.group(0), text)\n",
    "    # preserve code blocks\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        #skipping empty lines\n",
    "        if not stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "\n",
    "        # checking if code or pseudocode\n",
    "        is_code_line = (\n",
    "            len(line) - len(line.lstrip()) >= 4 or  # 4+ space indent\n",
    "            stripped.startswith('//') or  # comment\n",
    "            re.match(r'^(if|for|while|return|else)\\b', stripped, re.IGNORECASE) or\n",
    "            re.match(r'^[A-Z][A-Z\\-]+\\(', stripped)  # FUNCTION-NAME(\n",
    "        )\n",
    "\n",
    "        if is_code_line:\n",
    "            in_code_block = True\n",
    "            # keeping the line with a marker\n",
    "            processed_lines.append('__CODELINE__' + line)\n",
    "        else:\n",
    "            # checking if we just exited a code block\n",
    "            if in_code_block:\n",
    "                processed_lines.append('__CODEEND__')\n",
    "                in_code_block = False\n",
    "            processed_lines.append(line)\n",
    "\n",
    "    text = '\\n'.join(processed_lines)\n",
    "\n",
    "    #paragraph joining for non-code text where we join lines that are part of the same paragraph\n",
    "    text = re.sub(\n",
    "        r'(?<!__CODELINE__)(?<!__CODEEND__)(?<!\\n)\\n(?!__CODELINE__)(?!__CODEEND__)(?!\\n)(?![A-Z])', ' ', text)\n",
    "    #cleaning up markers\n",
    "    text = text.replace('__CODELINE__', '')\n",
    "    text = text.replace('__CODEEND__', '\\n')\n",
    "    #removing excessive blank lines (3+) but keep double newlines for sections\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    #cleaning up multiple spaces (but not at line start - that's indentation)\n",
    "    text = re.sub(r'([^\\n]) {2,}', r'\\1 ', text)\n",
    "    #fixing spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    #removing standalone page numbers (just digits on their own line)\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    #removing common header and footer patterns\n",
    "    text = re.sub(r'^(Chapter|Section)\\s+\\d+.*$', '',\n",
    "                  text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#detecting block of algorithm in clrs\n",
    "def is_algorithm_block(text: str) -> bool:\n",
    "    indicators = [\n",
    "        bool(re.search(r'\\b(if|then|else)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\b(for|while|do)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\breturn\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'^\\s*//.*', text, re.MULTILINE)),  # comments\n",
    "        bool(re.search(r'[A-Z][A-Z\\-]+\\([^)]*\\)', text)),  # FUNCTION(...)\n",
    "        bool(re.search(r'A\\[\\s*\\d+', text)),  # Array notation A[1]\n",
    "        bool(re.search(r'←|:=|=', text)),  # Assignment operators\n",
    "    ]\n",
    "    # Need at least 2 indicators and text should be substantial\n",
    "    return sum(indicators) >= 2 and len(text.split()) > 10\n",
    "\n",
    "#section header text\n",
    "def is_section_header(text: str) -> bool:\n",
    "    text = text.strip()\n",
    "    if not text or len(text) > 100:\n",
    "        return False\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) > 15 or len(words) < 2:\n",
    "        return False\n",
    "    # Skip table of contents entries (have lots of dots)\n",
    "    if text.count('.') > 3:\n",
    "        return False\n",
    "    # Skip if it's just \"Contents\" or roman numerals\n",
    "    if text.lower() in ['contents', 'preface', 'index', 'references']:\n",
    "        return False\n",
    "    # Likely a header if it doesn't end with period and isn't too long\n",
    "    return not text.endswith('.')\n",
    "\n",
    "#detecting table of contents\n",
    "def is_toc_or_front_matter(text: str) -> bool:\n",
    "    indicators = [\n",
    "        'contents' in text.lower()[:50],\n",
    "        'preface' in text.lower()[:50],\n",
    "        text.count('...') > 2,  # TOC dots\n",
    "        # Too many periods (TOC page numbers)\n",
    "        text.count('.') > len(text.split()) * 0.5,\n",
    "        # Roman numerals only\n",
    "        bool(re.search(r'^[ivxlcdm]+$', text.strip(), re.IGNORECASE)),\n",
    "    ]\n",
    "    return any(indicators) or len(text.split()) < 5\n",
    "\n",
    "#preserving chunk for contexts\n",
    "def smart_chunker(pages_and_texts: List[Dict],\n",
    "                  chunk_size: int = 1000,\n",
    "                  overlap: int = 200,\n",
    "                  skip_front_matter: bool = True) -> List[Dict]:\n",
    "    chunks = []\n",
    "\n",
    "    for page_data in pages_and_texts:\n",
    "        text = page_data['text']\n",
    "        page_num = page_data['page_number']\n",
    "\n",
    "        # Skip if this looks like front matter\n",
    "        if skip_front_matter and is_toc_or_front_matter(text):\n",
    "            continue\n",
    "        # Skip very short pages (likely artifacts)\n",
    "        if len(text.split()) < 10:\n",
    "            continue\n",
    "        # Split by double newlines to get paragraphs/blocks\n",
    "        blocks = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'has_algorithm': False,\n",
    "            'section_header': None\n",
    "        }\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "            if not block or len(block) < 10:\n",
    "                continue\n",
    "            if is_section_header(block):# if we have a current chunk, save it\n",
    "                if current_chunk and len(current_chunk.split()) > 10:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                # Start new chunk with this header\n",
    "                current_chunk = block + \"\\n\\n\"\n",
    "                current_chunk_metadata = {\n",
    "                    'page_number': page_num,\n",
    "                    'has_algorithm': False,\n",
    "                    'section_header': block\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Check if this is an algorithm block\n",
    "            block_has_algo = is_algorithm_block(block)\n",
    "            if block_has_algo:\n",
    "                current_chunk_metadata['has_algorithm'] = True\n",
    "\n",
    "                # If adding this would exceed chunk_size significantly and we have content, save current chunk\n",
    "                if len(current_chunk) + len(block) > chunk_size * 1.5 and len(current_chunk.split()) > 20:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap from previous\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Get more than we need to find clean word boundary\n",
    "                        overlap_start = max(\n",
    "                            0, len(current_chunk) - overlap - 50)\n",
    "                        overlap_text = current_chunk[overlap_start:]\n",
    "                        # Find first complete word (space followed by word followed by space)\n",
    "                        match = re.search(r'\\s+(\\S+\\s+)', overlap_text)\n",
    "                        if match:\n",
    "                            # Start from the beginning of that complete word\n",
    "                            overlap_text = overlap_text[match.start(1):]\n",
    "                        else:\n",
    "                            # Fallback: just find first space\n",
    "                            space_idx = overlap_text.find(' ')\n",
    "                            if space_idx > 0:\n",
    "                                overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': True,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    # Add algorithm block to current chunk (keep it together!)\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "            else:\n",
    "                # Regular text block\n",
    "                if len(current_chunk) + len(block) > chunk_size and len(current_chunk.split()) > 20:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Find word boundary for clean overlap\n",
    "                        overlap_text = current_chunk[-overlap:]\n",
    "                        # Adjust to start at word boundary\n",
    "                        space_idx = overlap_text.find(' ')\n",
    "                        if space_idx > 0:\n",
    "                            overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': False,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "\n",
    "        # Save the last chunk from this page if it's substantial\n",
    "        if current_chunk.strip() and len(current_chunk.split()) > 10:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'page_number': current_chunk_metadata['page_number'],\n",
    "                'chunk_char_count': len(current_chunk),\n",
    "                'chunk_word_count': len(current_chunk.split()),\n",
    "                'chunk_token_count': len(current_chunk) / 4,\n",
    "                'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                'section_header': current_chunk_metadata['section_header']\n",
    "            })\n",
    "\n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = i\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669acc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(pdf_path: str, page_offset: int = 0) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc), desc=\"Reading PDF pages\", total=len(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        # Skip completely empty pages\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - page_offset,\n",
    "            \"pdf_page_number\": page_number,  # Keep original for reference\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split()),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text) / 4,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "def find_content_start_page(pdf_path: str, sample_size: int = 50) -> int:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(min(sample_size, len(doc))):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "\n",
    "        #looking for indicators of actual content like chapter 1 and introduction etc.\n",
    "        if re.search(r'chapter\\s+1|^1\\s+introduction', text, re.IGNORECASE | re.MULTILINE):\n",
    "            print(f\"Found likely content start at PDF page {page_num}\")\n",
    "            print(f\"First 200 chars: {text[:200]}\")\n",
    "            return page_num\n",
    "\n",
    "    print(\"Could not automatically find content start. Please check manually.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d77a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found likely content start at PDF page 23\n",
      "First 200 chars: Introduction\n",
      "This part will start you thinking about designing and analyzing algorithms. It is\n",
      "intended to be a gentle introduction to how we specify algorithms, some of the\n",
      "design strategies we will \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7aa9415f7c4e8cafb8117238929572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/1313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 1306\n",
      "Total chunks: 1931\n",
      "Chunks with algorithms: 1260\n",
      "\n",
      "============================================================\n",
      "Chunk 50:\n",
      "Page: 14, Has algo: True\n",
      "Header: None\n",
      "Text preview: 2.3\n",
      "Designing algorithms 35 5 2 4 7 1 3 2 6 2 5 4 7 1 3 2 6 2 4 5 7 1 2 3 6 1 2 2 3 4 5 6 7 mergemerge mergesorted sequenceinitial sequencemerge mergemerge merge\n",
      "Figure 2.4\n",
      "The operation of merge sort on the array A D h5; 2; 4; 7; 1; 3; 2; 6i. The lengths of thesorted sequences being merged increase...\n",
      "\n",
      "============================================================\n",
      "Chunk 51:\n",
      "Page: 15, Has algo: True\n",
      "Header: None\n",
      "Text preview: Getting Startedthe original problem size is a power of 2. Each divide step then yields two subsequences of size exactly n=2. In Chapter 4, we shall see that this assumption doesnot affect the order of growth of the solution to the recurrence.\n",
      "We reason as follows to set up the recurrence for T.n/, t...\n",
      "\n",
      "============================================================\n",
      "Chunk 52:\n",
      "Page: 15, Has algo: True\n",
      "Header: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "Text preview: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "\n",
      "(2.1)\n",
      "In Chapter 4, we shall see the “master theorem,” which we can use to showthat T.n/ is ‚.n lg n/, where lg n stands for log2 n. Because the logarithm function grows more slowly than any linear function, for large enough inputs, mergesort, with its ‚.n lg n/ running tim...\n"
     ]
    }
   ],
   "source": [
    "content_start = find_content_start_page(pdf_path)\n",
    "pages_and_texts = open_and_read_pdf(pdf_path, page_offset=41)\n",
    "chunks = smart_chunker(pages_and_texts, chunk_size=1000,\n",
    "                       overlap=200, skip_front_matter=True)\n",
    "print(f\"Total pages: {len(pages_and_texts)}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\n",
    "    f\"Chunks with algorithms: {sum(1 for c in chunks if c['has_algorithm'])}\")\n",
    "for i in range(50, 53):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(\n",
    "        f\"Page: {chunks[i]['page_number']}, Has algo: {chunks[i]['has_algorithm']}\")\n",
    "    print(f\"Header: {chunks[i]['section_header']}\")\n",
    "    print(f\"Text preview: {chunks[i]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a944ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': -34,\n",
       "  'pdf_page_number': 7,\n",
       "  'page_char_count': 1057,\n",
       "  'page_word_count': 167,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 264.25,\n",
       "  'text': 'Contentsvii 12\\nBinary Search Trees 286 12.1 What is a binary search tree? 286 12.2 Querying a binary search tree 289 12.3 Insertion and deletion 294? 12.4 Randomly built binary search trees 299 13\\nRed-Black Trees 308 13.1 Properties of red-black trees 308 13.2 Rotations 312 13.3 Insertion 315 13.4 Deletion 323 14\\nAugmenting Data Structures 339 14.1 Dynamic order statistics 339 14.2 How to augment a data structure 345 14.3 Interval trees 348\\nIV\\nAdvanced Design and Analysis Techniques\\nIntroduction 357 15\\nDynamic Programming 359 15.1 Rod cutting 360 15.2 Matrix-chain multiplication 370 15.3 Elements of dynamic programming 378 15.4 Longest common subsequence 390 15.5 Optimal binary search trees 397 16\\nGreedy Algorithms 414 16.1 An activity-selection problem 415 16.2 Elements of the greedy strategy 423 16.3 Huffman codes 428? 16.4 Matroids and greedy methods 437? 16.5 A task-scheduling problem as a matroid 443 17\\nAmortized Analysis 451 17.1 Aggregate analysis 452 17.2 The accounting method 456 17.3 The potential method 459 17.4 Dynamic tables 463'},\n",
       " {'page_number': 984,\n",
       "  'pdf_page_number': 1025,\n",
       "  'page_char_count': 1658,\n",
       "  'page_word_count': 355,\n",
       "  'page_sentence_count_raw': 12,\n",
       "  'page_token_count': 414.5,\n",
       "  'text': '32.4\\nThe Knuth-Morris-Pratt algorithm 1005 1 2 3 4 5 6 7 0 0 1 2 3 0 1 ab ab ac a (a) ab ab ac aa ba ba ca ab ab ac aa ba ba ca (b) \" i\\nP Œi\\x8d \\x19Œi\\x8d\\nP5\\nP3\\nP1\\nP0 \\x19Œ5\\x8d D 3 \\x19Œ3\\x8d D 1 \\x19Œ1\\x8d D 0\\nFigure 32.11\\nAn illustration of Lemma 32.5 for the pattern P D ababaca and q D 5. (a) The \\x19 function for the given pattern. Since \\x19Œ5\\x8d D 3, \\x19Œ3\\x8d D 1, and \\x19Œ1\\x8d D 0, by iterating \\x19 we obtain \\x19\\x03Œ5\\x8d D f3; 1; 0g. (b) We slide the template containing the pattern P to the right and note when somepreﬁx Pk of P matches up with some proper sufﬁx of P5; we get matches when k D 3, 1, and 0. Inthe ﬁgure, the ﬁrst row gives P, and the dotted vertical line is drawn just after P5. Successive rowsshow all the shifts of P that cause some preﬁx Pk of P to match some sufﬁx of P5. Successfullymatched characters are shown shaded. Vertical lines connect aligned matching characters. Thus, fk W k < 5 and Pk = P5g D f3; 1; 0g. Lemma 32.5 claims that \\x19\\x03Œq\\x8d D fk W k < q and Pk = Pqgfor all q.\\nThe pseudocode below gives the Knuth-Morris-Pratt matching algorithm asthe procedure KMP-MATCHER. For the most part, the procedure follows from\\nFINITE-AUTOMATON-MATCHER, as we shall see. KMP-MATCHER calls the auxiliary procedure COMPUTE-PREFIX-FUNCTION to compute \\x19.\\nKMP-MATCHER.T; P / 1n D T:length 2 m D P:length 3 \\x19 D COMPUTE-PREFIX-FUNCTION.P / 4q D 0\\n// number of characters matched\\n\\nfor i D 1 to n\\n// scan the text from left to right\\n\\nwhile q > 0 and P Œq C 1\\x8d ¤ T Œi\\x8d\\n\\n7q D \\x19Œq\\x8d\\n// next character does not match\\n\\nif P Œq C 1\\x8d == T Œi\\x8d\\n\\n9q D q C 1\\n// next character matches\\n\\nif q == m\\n// is all of P matched?\\n\\n11print “Pattern occurs with shift” i \\x00 m 12 q D \\x19Œq\\x8d\\n// look for the next match'},\n",
       " {'page_number': 452,\n",
       "  'pdf_page_number': 493,\n",
       "  'page_char_count': 2339,\n",
       "  'page_word_count': 405,\n",
       "  'page_sentence_count_raw': 27,\n",
       "  'page_token_count': 584.75,\n",
       "  'text': 'Problems for Chapter 17 473 17-2\\nMaking binary search dynamic\\nBinary search of a sorted array takes logarithmic search time, but the time to inserta new element is linear in the size of the array. We can improve the time forinsertion by keeping several sorted arrays.\\nSpeciﬁcally, suppose that we wish to support SEARCH and INSERT on a setof n elements.\\nLet k D dlg.n C 1/e, and let the binary representation of nbe hnk\\x001; nk\\x002;:::; n0i. We have k sorted arrays A0; A1;:::; Ak\\x001, where fori D 0; 1;:::; k \\x00 1, the length of array Ai is 2i. Each array is either full or empty, depending on whether ni D 1 or ni D 0, respectively. The total number of elements held in all k arrays is therefore Pk\\x001iD0 ni 2i D n. Although each individualarray is sorted, elements in different arrays bear no particular relationship to eachother. a. Describe how to perform the SEARCH operation for this data structure. Analyzeits worst-case running time. b. Describe how to perform the INSERT operation. Analyze its worst-case andamortized running times. c. Discuss how to implement DELETE. 17-3\\nAmortized weight-balanced trees\\nConsider an ordinary binary search tree augmented by adding to each node x theattribute x:size giving the number of keys stored in the subtree rooted at x. Let ˛ be a constant in the range 1=2 \\x14 ˛ < 1. We say that a given node x is ˛-balancedif x:left:size \\x14 ˛ \\x01 x:size and x:right:size \\x14 ˛ \\x01 x:size. The tree as a wholeis ˛-balanced if every node in the tree is ˛-balanced. The following amortizedapproach to maintaining weight-balanced trees was suggested by G. Varghese. a. A 1=2-balanced tree is, in a sense, as balanced as it can be. Given a node xin an arbitrary binary search tree, show how to rebuild the subtree rooted at xso that it becomes 1=2-balanced. Your algorithm should run in time ‚.x:size/, and it can use O.x:size/ auxiliary storage. b. Show that performing a search in an n-node ˛-balanced binary search treetakes O.lg n/ worst-case time.\\nFor the remainder of this problem, assume that the constant ˛ is strictly greaterthan 1=2. Suppose that we implement INSERT and DELETE as usual for an n-nodebinary search tree, except that after every such operation, if any node in the treeis no longer ˛-balanced, then we “rebuild” the subtree rooted at the highest suchnode in the tree so that it becomes 1=2-balanced.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=3)# finding random text from random pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3028001",
   "metadata": {},
   "source": [
    "### 1.2 Getting Text Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddea2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>pdf_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>47.25</td>\n",
       "      <td>A L G O R I T H M S\\nI N T R O D U C T I O N T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Introduction to Algorithms\\nThird Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-38</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>41.25</td>\n",
       "      <td>Thomas H. Cormen\\nCharles E. Leiserson\\nRonald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37</td>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>127</td>\n",
       "      <td>13</td>\n",
       "      <td>220.75</td>\n",
       "      <td>c 2009 Massachusetts Institute of Technology\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "      <td>876</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>219.00</td>\n",
       "      <td>Contents\\nPreface xiii\\nI\\nFoundations\\nIntrod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  pdf_page_number  page_char_count  page_word_count  \\\n",
       "0          -41                0              189               92   \n",
       "1          -40                1               40                5   \n",
       "2          -38                3              165               23   \n",
       "3          -37                4              883              127   \n",
       "4          -36                5              876              134   \n",
       "\n",
       "   page_sentence_count_raw  page_token_count  \\\n",
       "0                        4             47.25   \n",
       "1                        1             10.00   \n",
       "2                        4             41.25   \n",
       "3                       13            220.75   \n",
       "4                        1            219.00   \n",
       "\n",
       "                                                text  \n",
       "0  A L G O R I T H M S\\nI N T R O D U C T I O N T...  \n",
       "1          Introduction to Algorithms\\nThird Edition  \n",
       "2  Thomas H. Cormen\\nCharles E. Leiserson\\nRonald...  \n",
       "3  c 2009 Massachusetts Institute of Technology\\n...  \n",
       "4  Contents\\nPreface xiii\\nI\\nFoundations\\nIntrod...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40fcfc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>pdf_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>615.66</td>\n",
       "      <td>656.66</td>\n",
       "      <td>1886.35</td>\n",
       "      <td>333.99</td>\n",
       "      <td>13.02</td>\n",
       "      <td>471.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>378.27</td>\n",
       "      <td>378.27</td>\n",
       "      <td>562.75</td>\n",
       "      <td>94.74</td>\n",
       "      <td>9.72</td>\n",
       "      <td>140.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>288.25</td>\n",
       "      <td>329.25</td>\n",
       "      <td>1547.25</td>\n",
       "      <td>279.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>386.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>615.50</td>\n",
       "      <td>656.50</td>\n",
       "      <td>1918.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>479.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>942.75</td>\n",
       "      <td>983.75</td>\n",
       "      <td>2282.25</td>\n",
       "      <td>396.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>570.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1271.00</td>\n",
       "      <td>1312.00</td>\n",
       "      <td>3225.00</td>\n",
       "      <td>570.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>806.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  pdf_page_number  page_char_count  page_word_count  \\\n",
       "count      1306.00          1306.00          1306.00          1306.00   \n",
       "mean        615.66           656.66          1886.35           333.99   \n",
       "std         378.27           378.27           562.75            94.74   \n",
       "min         -41.00             0.00            13.00             2.00   \n",
       "25%         288.25           329.25          1547.25           279.00   \n",
       "50%         615.50           656.50          1918.00           339.00   \n",
       "75%         942.75           983.75          2282.25           396.00   \n",
       "max        1271.00          1312.00          3225.00           570.00   \n",
       "\n",
       "       page_sentence_count_raw  page_token_count  \n",
       "count                  1306.00           1306.00  \n",
       "mean                     13.02            471.59  \n",
       "std                       9.72            140.69  \n",
       "min                       1.00              3.25  \n",
       "25%                       9.00            386.81  \n",
       "50%                      12.00            479.50  \n",
       "75%                      15.00            570.56  \n",
       "max                     103.00            806.25  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)# getting basic statistics of the dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
