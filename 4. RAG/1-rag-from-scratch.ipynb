{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8525a7",
   "metadata": {},
   "source": [
    "# Creating Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1305a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67944",
   "metadata": {},
   "source": [
    "### 1.1 Importing PDF Document for our Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import random\n",
    "import requests\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict  # for type hints\n",
    "from spacy.lang.en import English\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d880ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb454ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf. Proceeding to read the file.\n"
     ]
    }
   ],
   "source": [
    "# getting pdf documents from local system\n",
    "pdf_path = \"/Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf\"\n",
    "\n",
    "# download pdf if not present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Given path {pdf_path} does not exist. Downloading the pdf file!!!\")\n",
    "    url = \"https://www.cs.mcgill.ca/~akroit/math/compsci/Cormen%20Introduction%20to%20Algorithms.pdf\"\n",
    "    \n",
    "    filename = pdf_path\n",
    "    response = requests.get(url)# download the file\n",
    "    \n",
    "    if response.status_code == 200: # check if the download was successful\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)# save the file\n",
    "        print(f\"File downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists at {pdf_path}. Proceeding to read the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866fa3",
   "metadata": {},
   "source": [
    "Sine now we have imported our file now next step is to preprocess the text as we read it. We have imported the pages of book in the `file_path` and now we can open and read it with `PyMuPDF` by typing command `import fitz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0082351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    #fixing hyphenated words split across lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # fixing words split across lines WITHOUT hyphens\n",
    "    text = re.sub(r'(\\w+)\\s*\\n\\s*(\\w+)', lambda m: m.group(1) + m.group(2) if m.group(2)[0].islower() else m.group(0), text)\n",
    "    # preserve code blocks\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        #skipping empty lines\n",
    "        if not stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "\n",
    "        # checking if code or pseudocode\n",
    "        is_code_line = (\n",
    "            len(line) - len(line.lstrip()) >= 4 or  # 4+ space indent\n",
    "            stripped.startswith('//') or  # comment\n",
    "            re.match(r'^(if|for|while|return|else)\\b', stripped, re.IGNORECASE) or\n",
    "            re.match(r'^[A-Z][A-Z\\-]+\\(', stripped)  # FUNCTION-NAME(\n",
    "        )\n",
    "\n",
    "        if is_code_line:\n",
    "            in_code_block = True\n",
    "            # keeping the line with a marker\n",
    "            processed_lines.append('__CODELINE__' + line)\n",
    "        else:\n",
    "            # checking if we just exited a code block\n",
    "            if in_code_block:\n",
    "                processed_lines.append('__CODEEND__')\n",
    "                in_code_block = False\n",
    "            processed_lines.append(line)\n",
    "\n",
    "    text = '\\n'.join(processed_lines)\n",
    "\n",
    "    #paragraph joining for non-code text where we join lines that are part of the same paragraph\n",
    "    text = re.sub(\n",
    "        r'(?<!__CODELINE__)(?<!__CODEEND__)(?<!\\n)\\n(?!__CODELINE__)(?!__CODEEND__)(?!\\n)(?![A-Z])', ' ', text)\n",
    "    #cleaning up markers\n",
    "    text = text.replace('__CODELINE__', '')\n",
    "    text = text.replace('__CODEEND__', '\\n')\n",
    "    #removing excessive blank lines (3+) but keep double newlines for sections\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    #cleaning up multiple spaces (but not at line start - that's indentation)\n",
    "    text = re.sub(r'([^\\n]) {2,}', r'\\1 ', text)\n",
    "    #fixing spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    #removing standalone page numbers (just digits on their own line)\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    #removing common header and footer patterns\n",
    "    text = re.sub(r'^(Chapter|Section)\\s+\\d+.*$', '',\n",
    "                  text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#detecting block of algorithm in clrs\n",
    "def is_algorithm_block(text: str) -> bool:\n",
    "    indicators = [\n",
    "        bool(re.search(r'\\b(if|then|else)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\b(for|while|do)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\breturn\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'^\\s*//.*', text, re.MULTILINE)),  # comments\n",
    "        bool(re.search(r'[A-Z][A-Z\\-]+\\([^)]*\\)', text)),  # FUNCTION(...)\n",
    "        bool(re.search(r'A\\[\\s*\\d+', text)),  # Array notation A[1]\n",
    "        bool(re.search(r'←|:=|=', text)),  # Assignment operators\n",
    "    ]\n",
    "    # Need at least 2 indicators and text should be substantial\n",
    "    return sum(indicators) >= 2 and len(text.split()) > 10\n",
    "\n",
    "#section header text\n",
    "def is_section_header(text: str) -> bool:\n",
    "    text = text.strip()\n",
    "    if not text or len(text) > 100:\n",
    "        return False\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) > 15 or len(words) < 2:\n",
    "        return False\n",
    "    # Skip table of contents entries (have lots of dots)\n",
    "    if text.count('.') > 3:\n",
    "        return False\n",
    "    # Skip if it's just \"Contents\" or roman numerals\n",
    "    if text.lower() in ['contents', 'preface', 'index', 'references']:\n",
    "        return False\n",
    "    # Likely a header if it doesn't end with period and isn't too long\n",
    "    return not text.endswith('.')\n",
    "\n",
    "#detecting table of contents\n",
    "def is_toc_or_front_matter(text: str) -> bool:\n",
    "    indicators = [\n",
    "        'contents' in text.lower()[:50],\n",
    "        'preface' in text.lower()[:50],\n",
    "        text.count('...') > 2,  # TOC dots\n",
    "        # Too many periods (TOC page numbers)\n",
    "        text.count('.') > len(text.split()) * 0.5,\n",
    "        # Roman numerals only\n",
    "        bool(re.search(r'^[ivxlcdm]+$', text.strip(), re.IGNORECASE)),\n",
    "    ]\n",
    "    return any(indicators) or len(text.split()) < 5\n",
    "\n",
    "#preserving chunk for contexts\n",
    "def smart_chunker(pages_and_texts: List[Dict],\n",
    "                  chunk_size: int = 1000,\n",
    "                  overlap: int = 200,\n",
    "                  skip_front_matter: bool = True) -> List[Dict]:\n",
    "    chunks = []\n",
    "\n",
    "    for page_data in pages_and_texts:\n",
    "        text = page_data['text']\n",
    "        page_num = page_data['page_number']\n",
    "\n",
    "        # Skip if this looks like front matter\n",
    "        if skip_front_matter and is_toc_or_front_matter(text):\n",
    "            continue\n",
    "        # Skip very short pages (likely artifacts)\n",
    "        if len(text.split()) < 10:\n",
    "            continue\n",
    "        # Split by double newlines to get paragraphs/blocks\n",
    "        blocks = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'has_algorithm': False,\n",
    "            'section_header': None\n",
    "        }\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "            if not block or len(block) < 10:\n",
    "                continue\n",
    "            if is_section_header(block):# if we have a current chunk, save it\n",
    "                if current_chunk and len(current_chunk.split()) > 10:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                # Start new chunk with this header\n",
    "                current_chunk = block + \"\\n\\n\"\n",
    "                current_chunk_metadata = {\n",
    "                    'page_number': page_num,\n",
    "                    'has_algorithm': False,\n",
    "                    'section_header': block\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Check if this is an algorithm block\n",
    "            block_has_algo = is_algorithm_block(block)\n",
    "            if block_has_algo:\n",
    "                current_chunk_metadata['has_algorithm'] = True\n",
    "\n",
    "                # If adding this would exceed chunk_size significantly and we have content, save current chunk\n",
    "                if len(current_chunk) + len(block) > chunk_size * 1.5 and len(current_chunk.split()) > 20:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap from previous\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Get more than we need to find clean word boundary\n",
    "                        overlap_start = max(\n",
    "                            0, len(current_chunk) - overlap - 50)\n",
    "                        overlap_text = current_chunk[overlap_start:]\n",
    "                        # Find first complete word (space followed by word followed by space)\n",
    "                        match = re.search(r'\\s+(\\S+\\s+)', overlap_text)\n",
    "                        if match:\n",
    "                            # Start from the beginning of that complete word\n",
    "                            overlap_text = overlap_text[match.start(1):]\n",
    "                        else:\n",
    "                            # Fallback: just find first space\n",
    "                            space_idx = overlap_text.find(' ')\n",
    "                            if space_idx > 0:\n",
    "                                overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': True,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    # Add algorithm block to current chunk (keep it together!)\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "            else:\n",
    "                # Regular text block\n",
    "                if len(current_chunk) + len(block) > chunk_size and len(current_chunk.split()) > 20:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Find word boundary for clean overlap\n",
    "                        overlap_text = current_chunk[-overlap:]\n",
    "                        # Adjust to start at word boundary\n",
    "                        space_idx = overlap_text.find(' ')\n",
    "                        if space_idx > 0:\n",
    "                            overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': False,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "\n",
    "        # Save the last chunk from this page if it's substantial\n",
    "        if current_chunk.strip() and len(current_chunk.split()) > 10:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'page_number': current_chunk_metadata['page_number'],\n",
    "                'chunk_char_count': len(current_chunk),\n",
    "                'chunk_word_count': len(current_chunk.split()),\n",
    "                'chunk_token_count': len(current_chunk) / 4,\n",
    "                'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                'section_header': current_chunk_metadata['section_header']\n",
    "            })\n",
    "\n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = i\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669acc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(pdf_path: str, page_offset: int = 0) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc), desc=\"Reading PDF pages\", total=len(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        # Skip completely empty pages\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - page_offset,\n",
    "            \"pdf_page_number\": page_number,  # Keep original for reference\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split()),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text) / 4,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "def find_content_start_page(pdf_path: str, sample_size: int = 50) -> int:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(min(sample_size, len(doc))):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "\n",
    "        #looking for indicators of actual content like chapter 1 and introduction etc.\n",
    "        if re.search(r'chapter\\s+1|^1\\s+introduction', text, re.IGNORECASE | re.MULTILINE):\n",
    "            print(f\"Found likely content start at PDF page {page_num}\")\n",
    "            print(f\"First 200 chars: {text[:200]}\")\n",
    "            return page_num\n",
    "\n",
    "    print(\"Could not automatically find content start. Please check manually.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d77a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found likely content start at PDF page 23\n",
      "First 200 chars: Introduction\n",
      "This part will start you thinking about designing and analyzing algorithms. It is\n",
      "intended to be a gentle introduction to how we specify algorithms, some of the\n",
      "design strategies we will \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e405967abc47439caf695994539f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/1313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 1306\n",
      "Total chunks: 1931\n",
      "Chunks with algorithms: 1260\n",
      "\n",
      "============================================================\n",
      "Chunk 50:\n",
      "Page: 14, Has algo: True\n",
      "Header: None\n",
      "Text preview: 2.3\n",
      "Designing algorithms 35 5 2 4 7 1 3 2 6 2 5 4 7 1 3 2 6 2 4 5 7 1 2 3 6 1 2 2 3 4 5 6 7 mergemerge mergesorted sequenceinitial sequencemerge mergemerge merge\n",
      "Figure 2.4\n",
      "The operation of merge sort on the array A D h5; 2; 4; 7; 1; 3; 2; 6i. The lengths of thesorted sequences being merged increase...\n",
      "\n",
      "============================================================\n",
      "Chunk 51:\n",
      "Page: 15, Has algo: True\n",
      "Header: None\n",
      "Text preview: Getting Startedthe original problem size is a power of 2. Each divide step then yields two subsequences of size exactly n=2. In Chapter 4, we shall see that this assumption doesnot affect the order of growth of the solution to the recurrence.\n",
      "We reason as follows to set up the recurrence for T.n/, t...\n",
      "\n",
      "============================================================\n",
      "Chunk 52:\n",
      "Page: 15, Has algo: True\n",
      "Header: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "Text preview: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "\n",
      "(2.1)\n",
      "In Chapter 4, we shall see the “master theorem,” which we can use to showthat T.n/ is ‚.n lg n/, where lg n stands for log2 n. Because the logarithm function grows more slowly than any linear function, for large enough inputs, mergesort, with its ‚.n lg n/ running tim...\n"
     ]
    }
   ],
   "source": [
    "content_start = find_content_start_page(pdf_path)\n",
    "pages_and_texts = open_and_read_pdf(pdf_path, page_offset=41)\n",
    "chunks = smart_chunker(pages_and_texts, chunk_size=1000,\n",
    "                       overlap=200, skip_front_matter=True)\n",
    "print(f\"Total pages: {len(pages_and_texts)}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\n",
    "    f\"Chunks with algorithms: {sum(1 for c in chunks if c['has_algorithm'])}\")\n",
    "for i in range(50, 53):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(\n",
    "        f\"Page: {chunks[i]['page_number']}, Has algo: {chunks[i]['has_algorithm']}\")\n",
    "    print(f\"Header: {chunks[i]['section_header']}\")\n",
    "    print(f\"Text preview: {chunks[i]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a944ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 156,\n",
       "  'pdf_page_number': 197,\n",
       "  'page_char_count': 2503,\n",
       "  'page_word_count': 439,\n",
       "  'page_sentence_count_raw': 15,\n",
       "  'page_token_count': 625.75,\n",
       "  'text': '7.2\\nPerformance of quicksort 177 n 0 n–1 (n–1)/2 – 1 (n–1)/2n (n–1)/2 (a) (b) (n–1)/2 Θ(n) Θ(n)\\nFigure 7.5 (a) Two levels of a recursion tree for quicksort. The partitioning at the root costs nand produces a “bad” split: two subarrays of sizes 0 and n \\x00 1. The partitioning of the subarray ofsize n \\x00 1 costs n \\x00 1 and produces a “good” split: subarrays of size.n \\x00 1/=2 \\x00 1 and.n \\x00 1/=2. (b) A single level of a recursion tree that is very well balanced. In both parts, the partitioning cost forthe subproblems shown with elliptical shading is ‚.n/. Yet the subproblems remaining to be solvedin (a), shown with square shading, are no larger than the corresponding subproblems remaining to besolved in (b).\\nIntuition for the average case\\nTo develop a clear notion of the randomized behavior of quicksort, we must makean assumption about how frequently we expect to encounter the various inputs.\\nThe behavior of quicksort depends on the relative ordering of the values in thearray elements given as the input, and not by the particular values in the array. Asin our probabilistic analysis of the hiring problem in Section 5.2, we will assumefor now that all permutations of the input numbers are equally likely.\\nWhen we run quicksort on a random input array, the partitioning is highly unlikely to happen in the same way at every level, as our informal analysis has assumed. We expect that some of the splits will be reasonably well balanced andthat some will be fairly unbalanced. For example, Exercise 7.2-6 asks you to showthat about 80 percent of the time PARTITION produces a split that is more balancedthan 9 to 1, and about 20 percent of the time it produces a split that is less balancedthan 9 to 1.\\nIn the average case, PARTITION produces a mix of “good” and “bad” splits. In arecursion tree for an average-case execution of PARTITION, the good and bad splitsare distributed randomly throughout the tree. Suppose, for the sake of intuition, that the good and bad splits alternate levels in the tree, and that the good splitsare best-case splits and the bad splits are worst-case splits. Figure 7.5(a) showsthe splits at two consecutive levels in the recursion tree. At the root of the tree, the cost is n for partitioning, and the subarrays produced have sizes n \\x00 1 and 0: the worst case. At the next level, the subarray of size n \\x00 1 undergoes best-casepartitioning into subarrays of size.n \\x00 1/=2 \\x00 1 and.n \\x00 1/=2. Let’s assume thatthe boundary-condition cost is 1 for the subarray of size 0.'},\n",
       " {'page_number': 972,\n",
       "  'pdf_page_number': 1013,\n",
       "  'page_char_count': 2035,\n",
       "  'page_word_count': 422,\n",
       "  'page_sentence_count_raw': 12,\n",
       "  'page_token_count': 508.75,\n",
       "  'text': '32.2\\nThe Rabin-Karp algorithm 993\\nP Œ1:: m\\x8d D T Œs C 1:: s C m\\x8d. If q is large enough, then we hope that spurioushits occur infrequently enough that the cost of the extra checking is low.\\nThe following procedure makes these ideas precise. The inputs to the procedureare the text T, the pattern P, the radix d to use (which is typically taken to be j†j), and the prime q to use.\\nRABIN-KARP-MATCHER.T; P; d; q/ 1n D T:length 2 m D P:length 3 h D d m\\x001 mod q 4 p D 0 5 t0 D 0 6\\nfor i D 1 to m\\n// preprocessing\\n\\n7 p D.dp C P Œi\\x8d/ mod q 8 t0 D.dt0 C T Œi\\x8d/ mod q 9\\nfor s D 0 to n \\x00 m\\n// matching\\n\\nif p == ts\\n\\nif P Œ1:: m\\x8d == T Œs C 1:: s C m\\x8d\\n\\n12print “Pattern occurs with shift” s 13\\nif s < n \\x00 m\\n\\n14 tsC1 D.d.ts \\x00 T Œs C 1\\x8dh/ C T Œs C m C 1\\x8d/ mod q\\nThe procedure RABIN-KARP-MATCHER works as follows. All characters areinterpreted as radix-d digits. The subscripts on t are provided only for clarity; theprogram works correctly if all the subscripts are dropped. Line 3 initializes h to thevalue of the high-order digit position of an m-digit window. Lines 4–8 compute pas the value of P Œ1:: m\\x8d mod q and t0 as the value of T Œ1:: m\\x8d mod q. The forloop of lines 9–14 iterates through all possible shifts s, maintaining the followinginvariant:\\nWhenever line 10 is executed, ts D T Œs C 1:: s C m\\x8d mod q.\\nIf p D ts in line 10 (a “hit”), then line 11 checks to see whether P Œ1:: m\\x8d D\\n\\nT Œs C1:: s Cm\\x8d in order to rule out the possibility of a spurious hit. Line 12 printsout any valid shifts that are found. If s < n \\x00 m (checked in line 13), then the forloop will execute at least one more time, and so line 14 ﬁrst executes to ensure thatthe loop invariant holds when we get back to line 10. Line 14 computes the valueof tsC1 mod q from the value of ts mod q in constant time using equation (32.2) directly.\\nRABIN-KARP-MATCHER takes ‚.m/ preprocessing time, and its matching timeis ‚..n \\x00 m C 1/m/ in the worst case, since (like the naive string-matching algorithm) the Rabin-Karp algorithm explicitly veriﬁes every valid shift. If P D am'},\n",
       " {'page_number': 525,\n",
       "  'pdf_page_number': 566,\n",
       "  'page_char_count': 1735,\n",
       "  'page_word_count': 336,\n",
       "  'page_sentence_count_raw': 15,\n",
       "  'page_token_count': 433.75,\n",
       "  'text': 'Figure 20.5\\nThe information in a \\x17EB.u/ tree when u > 2. The structure contains the universe size u, elements min and max, a pointer summary to a \\x17EB. \"pu/ tree, and an arrayclusterŒ0:: \"pu \\x00 1\\x8d of \"pu pointers to \\x17EB. #pu/ trees. ger—that is, if u is an odd power of 2 (u D 22kC1 for some integer k \\x15 0)—thenwe will divide the lg u bits of a number into the most signiﬁcant d.lg u/=2e bits andthe least signiﬁcant b.lg u/=2c bits. For convenience, we denote 2d.lg u/=2e (the “upper square root” of u) by \"pu and 2b.lg u/=2c (the “lower square root” of u) by #pu, so that u D \"pu \\x01 #pu and, when u is an even power of 2 (u D 22k for someinteger k), \"pu D #pu D pu. Because we now allow u to be an odd power of 2, we must redeﬁne our helpful functions from Section 20.2: high.x/\\nD \\x04 x= #pu ˘; low.x/\\nDx mod #pu; index.x; y/\\nDx #pu C y: 20.3.1van Emde Boas trees\\nThe van Emde Boas tree, or vEB tree, modiﬁes the proto-vEB structure. Wedenote a vEB tree with a universe size of u as \\x17EB.u/ and, unless u equals thebase size of 2, the attribute summary points to a \\x17EB. \"pu/ tree and the arrayclusterŒ0:: \"pu \\x00 1\\x8d points to \"pu \\x17EB. #pu/ trees. As Figure 20.5 illustrates, avEB tree contains two attributes not found in a proto-vEB structure: \\x0f min stores the minimum element in the vEB tree, and \\x0f max stores the maximum element in the vEB tree.\\nFurthermore, the element stored in min does not appear in any of the recursive \\x17EB. #pu/ trees that the cluster array points to. The elements stored in a \\x17EB.u/ tree V, therefore, are V:min plus all the elements recursively stored inthe \\x17EB. #pu/ trees pointed to by V:clusterŒ0:: \"pu \\x00 1\\x8d. Note that when a vEBtree contains two or more elements, we treat min and max differently: the element'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_texts, k=3)# finding random text from random pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3028001",
   "metadata": {},
   "source": [
    "### 1.2 Getting Text Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddea2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>pdf_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>47.25</td>\n",
       "      <td>A L G O R I T H M S\\nI N T R O D U C T I O N T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Introduction to Algorithms\\nThird Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-38</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>41.25</td>\n",
       "      <td>Thomas H. Cormen\\nCharles E. Leiserson\\nRonald...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37</td>\n",
       "      <td>4</td>\n",
       "      <td>883</td>\n",
       "      <td>127</td>\n",
       "      <td>13</td>\n",
       "      <td>220.75</td>\n",
       "      <td>c 2009 Massachusetts Institute of Technology\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "      <td>876</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>219.00</td>\n",
       "      <td>Contents\\nPreface xiii\\nI\\nFoundations\\nIntrod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  pdf_page_number  page_char_count  page_word_count  \\\n",
       "0          -41                0              189               92   \n",
       "1          -40                1               40                5   \n",
       "2          -38                3              165               23   \n",
       "3          -37                4              883              127   \n",
       "4          -36                5              876              134   \n",
       "\n",
       "   page_sentence_count_raw  page_token_count  \\\n",
       "0                        4             47.25   \n",
       "1                        1             10.00   \n",
       "2                        4             41.25   \n",
       "3                       13            220.75   \n",
       "4                        1            219.00   \n",
       "\n",
       "                                                text  \n",
       "0  A L G O R I T H M S\\nI N T R O D U C T I O N T...  \n",
       "1          Introduction to Algorithms\\nThird Edition  \n",
       "2  Thomas H. Cormen\\nCharles E. Leiserson\\nRonald...  \n",
       "3  c 2009 Massachusetts Institute of Technology\\n...  \n",
       "4  Contents\\nPreface xiii\\nI\\nFoundations\\nIntrod...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fcfc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>pdf_page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "      <td>1306.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>615.66</td>\n",
       "      <td>656.66</td>\n",
       "      <td>1886.35</td>\n",
       "      <td>333.99</td>\n",
       "      <td>13.02</td>\n",
       "      <td>471.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>378.27</td>\n",
       "      <td>378.27</td>\n",
       "      <td>562.75</td>\n",
       "      <td>94.74</td>\n",
       "      <td>9.72</td>\n",
       "      <td>140.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>288.25</td>\n",
       "      <td>329.25</td>\n",
       "      <td>1547.25</td>\n",
       "      <td>279.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>386.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>615.50</td>\n",
       "      <td>656.50</td>\n",
       "      <td>1918.00</td>\n",
       "      <td>339.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>479.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>942.75</td>\n",
       "      <td>983.75</td>\n",
       "      <td>2282.25</td>\n",
       "      <td>396.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>570.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1271.00</td>\n",
       "      <td>1312.00</td>\n",
       "      <td>3225.00</td>\n",
       "      <td>570.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>806.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  pdf_page_number  page_char_count  page_word_count  \\\n",
       "count      1306.00          1306.00          1306.00          1306.00   \n",
       "mean        615.66           656.66          1886.35           333.99   \n",
       "std         378.27           378.27           562.75            94.74   \n",
       "min         -41.00             0.00            13.00             2.00   \n",
       "25%         288.25           329.25          1547.25           279.00   \n",
       "50%         615.50           656.50          1918.00           339.00   \n",
       "75%         942.75           983.75          2282.25           396.00   \n",
       "max        1271.00          1312.00          3225.00           570.00   \n",
       "\n",
       "       page_sentence_count_raw  page_token_count  \n",
       "count                  1306.00           1306.00  \n",
       "mean                     13.02            471.59  \n",
       "std                       9.72            140.69  \n",
       "min                       1.00              3.25  \n",
       "25%                       9.00            386.81  \n",
       "50%                      12.00            479.50  \n",
       "75%                      15.00            570.56  \n",
       "max                     103.00            806.25  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().round(2)# getting basic statistics of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d31e8",
   "metadata": {},
   "source": [
    "### 1.3 Splitting Sentences\n",
    "\n",
    "The purpose of splitting our texts into smaller groups is that we want them to fit into our embedding model context window which is having 384 tokens limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de232fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence chunk size set to: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sentence_chunk_size = 10# setting chunk size for sentences\n",
    "print(f\"Sentence chunk size set to: {num_sentence_chunk_size}\")\n",
    "\n",
    "#creating function to split text into sentence chunks\n",
    "def split_list(input_list: list[str], slice_size:int = num_sentence_chunk_size) -> list[list[str]]:    \n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5853c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
