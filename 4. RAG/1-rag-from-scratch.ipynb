{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8525a7",
   "metadata": {},
   "source": [
    "# Creating Local RAG Pipeline from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1305a8",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Embedding Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67944",
   "metadata": {},
   "source": [
    "### 1.1 Importing PDF Document for our Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict# for type hints\n",
    "from spacy.lang.en import English\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb454ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf. Proceeding to read the file.\n"
     ]
    }
   ],
   "source": [
    "# getting pdf documents from local system\n",
    "pdf_path = \"/Users/adityamishra/Documents/Machine Learning Tutorial/4. RAG/clrs.pdf\"\n",
    "\n",
    "# download pdf if not present\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"Given path {pdf_path} does not exist. Downloading the pdf file!!!\")\n",
    "    url = \"https://www.cs.mcgill.ca/~akroit/math/compsci/Cormen%20Introduction%20to%20Algorithms.pdf\"\n",
    "    \n",
    "    filename = pdf_path\n",
    "    response = requests.get(url)# download the file\n",
    "    \n",
    "    if response.status_code == 200: # check if the download was successful\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)# save the file\n",
    "        print(f\"File downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists at {pdf_path}. Proceeding to read the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866fa3",
   "metadata": {},
   "source": [
    "Sine now we have imported our file now next step is to preprocess the text as we read it. We have imported the pages of book in the `file_path` and now we can open and read it with `PyMuPDF` by typing command `import fitz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0082351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    #fixing hyphenated words split across lines\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    # fixing words split across lines WITHOUT hyphens\n",
    "    text = re.sub(r'(\\w+)\\s*\\n\\s*(\\w+)', lambda m: m.group(1) + m.group(2) if m.group(2)[0].islower() else m.group(0), text)\n",
    "    # preserve code blocks\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "        #skipping empty lines\n",
    "        if not stripped:\n",
    "            processed_lines.append('')\n",
    "            continue\n",
    "\n",
    "        # checking if code or pseudocode\n",
    "        is_code_line = (\n",
    "            len(line) - len(line.lstrip()) >= 4 or  # 4+ space indent\n",
    "            stripped.startswith('//') or  # comment\n",
    "            re.match(r'^(if|for|while|return|else)\\b', stripped, re.IGNORECASE) or\n",
    "            re.match(r'^[A-Z][A-Z\\-]+\\(', stripped)  # FUNCTION-NAME(\n",
    "        )\n",
    "\n",
    "        if is_code_line:\n",
    "            in_code_block = True\n",
    "            # keeping the line with a marker\n",
    "            processed_lines.append('__CODELINE__' + line)\n",
    "        else:\n",
    "            # checking if we just exited a code block\n",
    "            if in_code_block:\n",
    "                processed_lines.append('__CODEEND__')\n",
    "                in_code_block = False\n",
    "            processed_lines.append(line)\n",
    "\n",
    "    text = '\\n'.join(processed_lines)\n",
    "\n",
    "    #paragraph joining for non-code text where we join lines that are part of the same paragraph\n",
    "    text = re.sub(\n",
    "        r'(?<!__CODELINE__)(?<!__CODEEND__)(?<!\\n)\\n(?!__CODELINE__)(?!__CODEEND__)(?!\\n)(?![A-Z])', ' ', text)\n",
    "    #cleaning up markers\n",
    "    text = text.replace('__CODELINE__', '')\n",
    "    text = text.replace('__CODEEND__', '\\n')\n",
    "    #removing excessive blank lines (3+) but keep double newlines for sections\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    #cleaning up multiple spaces (but not at line start - that's indentation)\n",
    "    text = re.sub(r'([^\\n]) {2,}', r'\\1 ', text)\n",
    "    #fixing spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    #removing standalone page numbers (just digits on their own line)\n",
    "    text = re.sub(r'^\\s*\\d{1,4}\\s*$', '', text, flags=re.MULTILINE)\n",
    "    #removing common header and footer patterns\n",
    "    text = re.sub(r'^(Chapter|Section)\\s+\\d+.*$', '',\n",
    "                  text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#detecting block of algorithm in clrs\n",
    "def is_algorithm_block(text: str) -> bool:\n",
    "    indicators = [\n",
    "        bool(re.search(r'\\b(if|then|else)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\b(for|while|do)\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'\\breturn\\b', text, re.IGNORECASE)),\n",
    "        bool(re.search(r'^\\s*//.*', text, re.MULTILINE)),  # comments\n",
    "        bool(re.search(r'[A-Z][A-Z\\-]+\\([^)]*\\)', text)),  # FUNCTION(...)\n",
    "        bool(re.search(r'A\\[\\s*\\d+', text)),  # Array notation A[1]\n",
    "        bool(re.search(r'←|:=|=', text)),  # Assignment operators\n",
    "    ]\n",
    "    # Need at least 2 indicators and text should be substantial\n",
    "    return sum(indicators) >= 2 and len(text.split()) > 10\n",
    "\n",
    "#section header text\n",
    "def is_section_header(text: str) -> bool:\n",
    "    text = text.strip()\n",
    "    if not text or len(text) > 100:\n",
    "        return False\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) > 15 or len(words) < 2:\n",
    "        return False\n",
    "    # Skip table of contents entries (have lots of dots)\n",
    "    if text.count('.') > 3:\n",
    "        return False\n",
    "    # Skip if it's just \"Contents\" or roman numerals\n",
    "    if text.lower() in ['contents', 'preface', 'index', 'references']:\n",
    "        return False\n",
    "    # Likely a header if it doesn't end with period and isn't too long\n",
    "    return not text.endswith('.')\n",
    "\n",
    "#detecting table of contents\n",
    "def is_toc_or_front_matter(text: str) -> bool:\n",
    "    indicators = [\n",
    "        'contents' in text.lower()[:50],\n",
    "        'preface' in text.lower()[:50],\n",
    "        text.count('...') > 2,  # TOC dots\n",
    "        # Too many periods (TOC page numbers)\n",
    "        text.count('.') > len(text.split()) * 0.5,\n",
    "        # Roman numerals only\n",
    "        bool(re.search(r'^[ivxlcdm]+$', text.strip(), re.IGNORECASE)),\n",
    "    ]\n",
    "    return any(indicators) or len(text.split()) < 5\n",
    "\n",
    "#preserving chunk for contexts\n",
    "def smart_chunker(pages_and_texts: List[Dict],\n",
    "                  chunk_size: int = 1000,\n",
    "                  overlap: int = 200,\n",
    "                  skip_front_matter: bool = True) -> List[Dict]:\n",
    "    chunks = []\n",
    "\n",
    "    for page_data in pages_and_texts:\n",
    "        text = page_data['text']\n",
    "        page_num = page_data['page_number']\n",
    "\n",
    "        # Skip if this looks like front matter\n",
    "        if skip_front_matter and is_toc_or_front_matter(text):\n",
    "            continue\n",
    "        # Skip very short pages (likely artifacts)\n",
    "        if len(text.split()) < 10:\n",
    "            continue\n",
    "        # Split by double newlines to get paragraphs/blocks\n",
    "        blocks = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_metadata = {\n",
    "            'page_number': page_num,\n",
    "            'has_algorithm': False,\n",
    "            'section_header': None\n",
    "        }\n",
    "        for block in blocks:\n",
    "            block = block.strip()\n",
    "            if not block or len(block) < 10:\n",
    "                continue\n",
    "            if is_section_header(block):# if we have a current chunk, save it\n",
    "                if current_chunk and len(current_chunk.split()) > 10:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                # Start new chunk with this header\n",
    "                current_chunk = block + \"\\n\\n\"\n",
    "                current_chunk_metadata = {\n",
    "                    'page_number': page_num,\n",
    "                    'has_algorithm': False,\n",
    "                    'section_header': block\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # Check if this is an algorithm block\n",
    "            block_has_algo = is_algorithm_block(block)\n",
    "            if block_has_algo:\n",
    "                current_chunk_metadata['has_algorithm'] = True\n",
    "\n",
    "                # If adding this would exceed chunk_size significantly and we have content, save current chunk\n",
    "                if len(current_chunk) + len(block) > chunk_size * 1.5 and len(current_chunk.split()) > 20:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap from previous\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Get more than we need to find clean word boundary\n",
    "                        overlap_start = max(\n",
    "                            0, len(current_chunk) - overlap - 50)\n",
    "                        overlap_text = current_chunk[overlap_start:]\n",
    "                        # Find first complete word (space followed by word followed by space)\n",
    "                        match = re.search(r'\\s+(\\S+\\s+)', overlap_text)\n",
    "                        if match:\n",
    "                            # Start from the beginning of that complete word\n",
    "                            overlap_text = overlap_text[match.start(1):]\n",
    "                        else:\n",
    "                            # Fallback: just find first space\n",
    "                            space_idx = overlap_text.find(' ')\n",
    "                            if space_idx > 0:\n",
    "                                overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': True,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    # Add algorithm block to current chunk (keep it together!)\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "            else:\n",
    "                # Regular text block\n",
    "                if len(current_chunk) + len(block) > chunk_size and len(current_chunk.split()) > 20:\n",
    "                    # Save current chunk\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'page_number': current_chunk_metadata['page_number'],\n",
    "                        'chunk_char_count': len(current_chunk),\n",
    "                        'chunk_word_count': len(current_chunk.split()),\n",
    "                        'chunk_token_count': len(current_chunk) / 4,\n",
    "                        'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    })\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    if len(current_chunk) > overlap:\n",
    "                        # Find word boundary for clean overlap\n",
    "                        overlap_text = current_chunk[-overlap:]\n",
    "                        # Adjust to start at word boundary\n",
    "                        space_idx = overlap_text.find(' ')\n",
    "                        if space_idx > 0:\n",
    "                            overlap_text = overlap_text[space_idx+1:]\n",
    "                    else:\n",
    "                        overlap_text = current_chunk\n",
    "                    current_chunk = overlap_text + \"\\n\\n\" + block + \"\\n\\n\"\n",
    "                    current_chunk_metadata = {\n",
    "                        'page_number': page_num,\n",
    "                        'has_algorithm': False,\n",
    "                        'section_header': current_chunk_metadata['section_header']\n",
    "                    }\n",
    "                else:\n",
    "                    current_chunk += block + \"\\n\\n\"\n",
    "\n",
    "        # Save the last chunk from this page if it's substantial\n",
    "        if current_chunk.strip() and len(current_chunk.split()) > 10:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'page_number': current_chunk_metadata['page_number'],\n",
    "                'chunk_char_count': len(current_chunk),\n",
    "                'chunk_word_count': len(current_chunk.split()),\n",
    "                'chunk_token_count': len(current_chunk) / 4,\n",
    "                'has_algorithm': current_chunk_metadata['has_algorithm'],\n",
    "                'section_header': current_chunk_metadata['section_header']\n",
    "            })\n",
    "\n",
    "    # Add chunk IDs\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_id'] = i\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "669acc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(pdf_path: str, page_offset: int = 0) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc), desc=\"Reading PDF pages\", total=len(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        # Skip completely empty pages\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        pages_and_texts.append({\n",
    "            \"page_number\": page_number - page_offset,\n",
    "            \"pdf_page_number\": page_number,  # Keep original for reference\n",
    "            \"page_char_count\": len(text),\n",
    "            \"page_word_count\": len(text.split()),\n",
    "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "            \"page_token_count\": len(text) / 4,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "def find_content_start_page(pdf_path: str, sample_size: int = 50) -> int:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(min(sample_size, len(doc))):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "\n",
    "        #looking for indicators of actual content like chapter 1 and introduction etc.\n",
    "        if re.search(r'chapter\\s+1|^1\\s+introduction', text, re.IGNORECASE | re.MULTILINE):\n",
    "            print(f\"Found likely content start at PDF page {page_num}\")\n",
    "            print(f\"First 200 chars: {text[:200]}\")\n",
    "            return page_num\n",
    "\n",
    "    print(\"Could not automatically find content start. Please check manually.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89d77a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found likely content start at PDF page 23\n",
      "First 200 chars: Introduction\n",
      "This part will start you thinking about designing and analyzing algorithms. It is\n",
      "intended to be a gentle introduction to how we specify algorithms, some of the\n",
      "design strategies we will \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df0cc9795f3493fa4118de1696d759c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDF pages:   0%|          | 0/1313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 1306\n",
      "Total chunks: 1931\n",
      "Chunks with algorithms: 1260\n",
      "\n",
      "============================================================\n",
      "Chunk 50:\n",
      "Page: 14, Has algo: True\n",
      "Header: None\n",
      "Text preview: 2.3\n",
      "Designing algorithms 35 5 2 4 7 1 3 2 6 2 5 4 7 1 3 2 6 2 4 5 7 1 2 3 6 1 2 2 3 4 5 6 7 mergemerge mergesorted sequenceinitial sequencemerge mergemerge merge\n",
      "Figure 2.4\n",
      "The operation of merge sort on the array A D h5; 2; 4; 7; 1; 3; 2; 6i. The lengths of thesorted sequences being merged increase...\n",
      "\n",
      "============================================================\n",
      "Chunk 51:\n",
      "Page: 15, Has algo: True\n",
      "Header: None\n",
      "Text preview: Getting Startedthe original problem size is a power of 2. Each divide step then yields two subsequences of size exactly n=2. In Chapter 4, we shall see that this assumption doesnot affect the order of growth of the solution to the recurrence.\n",
      "We reason as follows to set up the recurrence for T.n/, t...\n",
      "\n",
      "============================================================\n",
      "Chunk 52:\n",
      "Page: 15, Has algo: True\n",
      "Header: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "Text preview: 2T.n=2/ C ‚.n/\n",
      "if n > 1:\n",
      "\n",
      "(2.1)\n",
      "In Chapter 4, we shall see the “master theorem,” which we can use to showthat T.n/ is ‚.n lg n/, where lg n stands for log2 n. Because the logarithm function grows more slowly than any linear function, for large enough inputs, mergesort, with its ‚.n lg n/ running tim...\n"
     ]
    }
   ],
   "source": [
    "content_start = find_content_start_page(pdf_path)\n",
    "pages_and_texts = open_and_read_pdf(pdf_path, page_offset=41)\n",
    "chunks = smart_chunker(pages_and_texts, chunk_size=1000,\n",
    "                       overlap=200, skip_front_matter=True)\n",
    "print(f\"Total pages: {len(pages_and_texts)}\")\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\n",
    "    f\"Chunks with algorithms: {sum(1 for c in chunks if c['has_algorithm'])}\")\n",
    "for i in range(50, 53):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(\n",
    "        f\"Page: {chunks[i]['page_number']}, Has algo: {chunks[i]['has_algorithm']}\")\n",
    "    print(f\"Header: {chunks[i]['section_header']}\")\n",
    "    print(f\"Text preview: {chunks[i]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a944ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
